{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFAoaj9dMATJ"
      },
      "source": [
        "## Лабораторная работа №5 (Проведение исследований с градиентным бустингом)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmLUPmRlzZBj"
      },
      "source": [
        "## Классификация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVuBRo3dzfPD"
      },
      "source": [
        "# 2. Создание бейзлайна и оценка качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXks6TXI2BiS"
      },
      "source": [
        "**a. Обучение модели sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUM1q0RxP-uL",
        "outputId": "82c86522-be1d-4d49-fae1-60232fa9ddeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Градиентный бустинг Бейзлайн\n",
            "Accuracy: 0.9222\n",
            "Precision: 0.9412\n",
            "Recall: 0.6864\n",
            "F1-Score: 0.7938\n",
            "ROC-AUC: 0.9254\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Заполняем пропуски медианой\n",
        "df = df.fillna(df.median())\n",
        "\n",
        "# Разделяем на признаки и целевой класс\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучение\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = gb.predict(X_test)\n",
        "y_pred_proba = gb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Градиентный бустинг Бейзлайн\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-Eyq04b2MHZ"
      },
      "source": [
        "**b. Оценка качества модели**\n",
        "- Accuracy: 0.9222\n",
        "- Precision: 0.9412\n",
        "- Recall: 0.6864\n",
        "- F1-Score: 0.7938\n",
        "- ROC-AUC: 0.9254\n",
        "\n",
        "Градиентный бустинг показывает хорошие результаты, но хотя я ожидал немного большего."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjPhND9m1HNy"
      },
      "source": [
        "# 3. Улучшение бейзлайна"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1E-thlg2wNd"
      },
      "source": [
        "**a. Формулирование гипотез**\n",
        "\n",
        "\n",
        "**Гипотеза 1: Препроцессинг**\n",
        "\n",
        "Удалить явные выбросы по возрасту и стажу (person_age < 100, person_emp_length < 100), заполнить пропуски в числовых колонках медианой и перейти с Label Encoding на One-hot для категориальных признаков (person_home_ownership, loan_intent, loan_grade, cb_person_default_on_file). Хотя градиентный бустинг неплохо работает с Label Encoding, почему бы не попробовать One-hot.\n",
        "\n",
        "**Гипотеза 2: Подбор гиперпараметров**\n",
        "\n",
        "Найти оптимальные параметры градиентного бустинга с помощью кросс-валидации:\n",
        "- n_estimators (количество деревьев): 100, 200, 300\n",
        "- learning_rate (скорость обучения): 0.01, 0.05, 0.1\n",
        "- max_depth (глубина деревьев): 3, 5, 7\n",
        "- min_samples_split: 2, 5, 10\n",
        "\n",
        "**Гипотеза 3: Балансировка классов**\n",
        "\n",
        "Использовать параметр class_weight='balanced' или SMOTE на тренировочной выборке, чтобы модель лучше училась на дефолтах и улучшить Recall.\n",
        "\n",
        "**Гипотеза 4: Новые признаки**\n",
        "\n",
        "Добавить признаки income_to_loan (доход / сумма кредита) и age_emp_ratio (возраст / стаж) до One-hot encoding, чтобы дать модели более содержательную информацию о клиенте.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiM7NQst22El"
      },
      "source": [
        "**b. Проверка гипотез**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IKVcU9vRWhE",
        "outputId": "2da00f68-3533-4cfd-dca6-47af7f05dbe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "До удаления выбросов: 32581\n",
            "После удаления выбросов: 31679\n",
            "Количество признаков после One-hot: 26\n",
            "Гипотеза 1: препроцессинг\n",
            "Accuracy: 0.9350\n",
            "Precision: 0.9631\n",
            "Recall: 0.7260\n",
            "F1-Score: 0.8279\n",
            "ROC-AUC: 0.9334\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 1: препроцессинг (удаление выбросов + One-hot encoding)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Удаляем выбросы\n",
        "print(\"До удаления выбросов:\", len(df))\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "print(\"После удаления выбросов:\", len(df))\n",
        "\n",
        "# Заполняем пропуски только в числовых колонках\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# One-hot encoding для категориальных признаков\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем на признаки и целевой класс\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "print(f\"Количество признаков после One-hot: {X.shape[1]}\")\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = gb.predict(X_test)\n",
        "y_pred_proba = gb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Гипотеза 1: препроцессинг\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6JkXQTgSx-3"
      },
      "source": [
        "**Вывод по гипотезе 1:**\n",
        "\n",
        "Все метрики улучшились, препроцессинг как и раньше, работает безотказно.\n",
        "\n",
        "Гипотеза 1 подтверждена, можно использовать.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNi6jcnQS7Sj",
        "outputId": "7e50b726-af1f-4ae0-e5e6-36851f00b574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Проверка разных комбинаций параметров с кросс-валидацией (5-fold):\n",
            "\n",
            "1. n_est=100, lr=0.1, depth=3: F1=0.8049\n",
            "2. n_est=200, lr=0.1, depth=3: F1=0.8178\n",
            "3. n_est=200, lr=0.05, depth=5: F1=0.8270\n",
            "4. n_est=300, lr=0.05, depth=5: F1=0.8290\n",
            "5. n_est=200, lr=0.1, depth=5: F1=0.8312\n",
            "\n",
            "Лучшие параметры: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5}\n",
            "Лучший F1-Score (5-fold CV): 0.8312\n",
            "\n",
            "Гипотеза 2: подбор гиперпараметров\n",
            "Accuracy: 0.9402\n",
            "Precision: 0.9695\n",
            "Recall: 0.7458\n",
            "F1-Score: 0.8431\n",
            "ROC-AUC: 0.9524\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 2: подбор гиперпараметров\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Упрощённая сетка для быстрого перебора\n",
        "params_to_test = [\n",
        "    {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 3},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 5},\n",
        "    {'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 5},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5},\n",
        "]\n",
        "\n",
        "print(\"Проверка разных комбинаций параметров с кросс-валидацией (5-fold):\\n\")\n",
        "\n",
        "results = []\n",
        "for i, params in enumerate(params_to_test, 1):\n",
        "    gb = GradientBoostingClassifier(random_state=42, **params)\n",
        "    cv_scores = cross_val_score(gb, X_train, y_train, cv=5, scoring='f1')\n",
        "    mean_score = cv_scores.mean()\n",
        "    results.append({'params': params, 'mean_f1': mean_score})\n",
        "    print(f\"{i}. n_est={params['n_estimators']}, lr={params['learning_rate']}, depth={params['max_depth']}: F1={mean_score:.4f}\")\n",
        "\n",
        "# Находим лучшие параметры\n",
        "best_result = max(results, key=lambda x: x['mean_f1'])\n",
        "best_params = best_result['params']\n",
        "best_f1_cv = best_result['mean_f1']\n",
        "\n",
        "print(f\"\\nЛучшие параметры: {best_params}\")\n",
        "print(f\"Лучший F1-Score (5-fold CV): {best_f1_cv:.4f}\")\n",
        "\n",
        "# Обучаем с лучшими параметрами\n",
        "best_gb = GradientBoostingClassifier(random_state=42, **best_params)\n",
        "best_gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = best_gb.predict(X_test)\n",
        "y_pred_proba = best_gb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nГипотеза 2: подбор гиперпараметров\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6_DtQ52XVE4"
      },
      "source": [
        "**Выводы по гипотезе 2:**\n",
        "\n",
        "Подбор гиперпараметров дал улучшение по всем метрикам.\n",
        "\n",
        "Гипотеза 2 подтверждена, добавляем в улучшенный бейзлайн."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU9fHqviXdWk",
        "outputId": "25f3f1f8-4d6e-4ae9-89ad-daf3ac8f996f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Распределение классов ДО SMOTE:\n",
            "loan_status\n",
            "0    19883\n",
            "1     5460\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Распределение классов ПОСЛЕ SMOTE:\n",
            "loan_status\n",
            "0    19883\n",
            "1    19883\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Гипотеза 3: SMOTE\n",
            "Accuracy: 0.9372\n",
            "Precision: 0.9609\n",
            "Recall: 0.7385\n",
            "F1-Score: 0.8351\n",
            "ROC-AUC: 0.9451\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 3: балансировка классов (SMOTE)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "print(\"Распределение классов ДО SMOTE:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "# Применяем SMOTE на тренировочных данных\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nРаспределение классов ПОСЛЕ SMOTE:\")\n",
        "print(pd.Series(y_train_smote).value_counts())\n",
        "\n",
        "# Нормализуем данные после SMOTE\n",
        "scaler = StandardScaler()\n",
        "X_train_smote = scaler.fit_transform(X_train_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем\n",
        "gb_smote = GradientBoostingClassifier(random_state=42, **best_params)\n",
        "gb_smote.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "y_pred = gb_smote.predict(X_test_scaled)\n",
        "y_pred_proba = gb_smote.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nГипотеза 3: SMOTE\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWt4Hiu1X8ww"
      },
      "source": [
        "**Вывод по Гипотезе 3:**\n",
        "\n",
        "SMOTE не улучшил качество, все метрики ухудшились.\n",
        "\n",
        "Гипотеза 3 не подтверждена.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al_eAETZZBGc",
        "outputId": "8655bb8e-6612-4731-d424-7d814abe1774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Новые признаки созданы:\n",
            " - income_to_loan: min=1.20, max=1265.82\n",
            " - age_emp_ratio: min=1.38, max=74.00\n",
            "\n",
            "Количество признаков (без новых): 26\n",
            "Количество признаков (с новыми): 28\n",
            "\n",
            "Гипотеза 4: новые признаки\n",
            "Accuracy: 0.9389\n",
            "Precision: 0.9675\n",
            "Recall: 0.7414\n",
            "F1-Score: 0.8395\n",
            "ROC-AUC: 0.9511\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 4: новые признаки\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Удаляем выбросы\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "\n",
        "# Заполняем пропуски только в числовых колонках\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Создаём новые признаки до One-hot encoding\n",
        "df['income_to_loan'] = df['person_income'] / (df['loan_amnt'] + 1) # +1 чтобы избежать деления на 0\n",
        "df['age_emp_ratio'] = (df['person_age'] + 1) / (df['person_emp_length'] + 1) # +1 для безопасности\n",
        "\n",
        "print(f\"Новые признаки созданы:\")\n",
        "print(f\" - income_to_loan: min={df['income_to_loan'].min():.2f}, max={df['income_to_loan'].max():.2f}\")\n",
        "print(f\" - age_emp_ratio: min={df['age_emp_ratio'].min():.2f}, max={df['age_emp_ratio'].max():.2f}\")\n",
        "\n",
        "# One-hot encoding для категориальных признаков\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем на признаки и целевой класс\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "print(f\"\\nКоличество признаков (без новых): 26\")\n",
        "print(f\"Количество признаков (с новыми): {X.shape[1]}\")\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем\n",
        "gb_fe = GradientBoostingClassifier(random_state=42, **best_params)\n",
        "gb_fe.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb_fe.predict(X_test)\n",
        "y_pred_proba = gb_fe.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nГипотеза 4: новые признаки\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vIJrZDSZVK2"
      },
      "source": [
        "**Вывод по Гипотезе 4:**\n",
        "\n",
        "Новые признаки не улучшили качество, все метрики немного ухудшились, модель и так хорошо справлялась без них.\n",
        "\n",
        "Гипотеза 4 из рассмотрения исключаем.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hitY9ZW72-ts"
      },
      "source": [
        "**c., d. и h. Формирование улучшенного бейзлайна, обучение и оценка качества модели**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXtOwrVHaBjf",
        "outputId": "f67ef704-8a33-47fb-cd2f-f2d4cff2bae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Улучшенный бейзлайн Gradient Boosting\n",
            "Accuracy: 0.9402\n",
            "Precision: 0.9695\n",
            "Recall: 0.7458\n",
            "F1-Score: 0.8431\n",
            "ROC-AUC: 0.9524\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 1 (Препроцессинг) + Гипотеза 2 (лучшие параметры)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Улучшение 1: Удаляем выбросы\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "\n",
        "# Улучшение 2: Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Улучшение 3: One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем на признаки и целевой класс\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Улучшение 4: Лучшие параметры из гипотезы 2\n",
        "gb_improved = GradientBoostingClassifier(random_state=42, **best_params)\n",
        "gb_improved.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = gb_improved.predict(X_test)\n",
        "y_pred_proba = gb_improved.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Улучшенный бейзлайн Gradient Boosting\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmF7pxXn3ot1"
      },
      "source": [
        "**f. Сравнение результатов с пунктом 2**\n",
        "\n",
        "Улучшенный бейзлайн показал заметное улучшение по всем метрикам по сравнению с базовым. Accuracy выросла с 92.22% до 94.02%, Precision улучшилась с 94.12% до 96.95%, что означает меньше ложных тревог при предсказании дефолтов. Recall вырос с 68.64% до 74.58%, это самое значительное улучшение, модель стала ловить на 6% больше реальных дефолтов. F1-Score поднялся с 79.38% до 84.31%, а ROC-AUC выросла с 92.54% до 95.24%. Препроцессинг данных и подбор гиперпараметров дали хороший результат, модель стала значительно точнее и надёжнее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOupt5Ad3w7q"
      },
      "source": [
        "**g. Выводы**\n",
        "\n",
        "Все метрики выросли:\n",
        "- Accuracy: с 92.22% до 94.02%\n",
        "- Precision: с 94.12% до 96.95%\n",
        "- Recall: с 68.64% до 74.58%\n",
        "- F1-Score: с 79.38% до 84.31%\n",
        "- ROC-AUC: с 92.54% до 95.24%\n",
        "\n",
        "Самый важный результат это Recall, он вырос на 6%, теперь модель ловит почти 75% всех дефолтов вместо 69%. Препроцессинг с удалением выбросов и One-hot encoding помог модели лучше понимать данные. Подбор гиперпараметров через кросс-валидацию дал основной прирост качества. Градиентный бустинг оказался очень чувствителен к настройкам, правильные параметры критически важны, SMOTE и новые признаки не помогли, градиентный бустинг и так хорошо работает с несбалансированными данными и сам находит нужные закономерности. Подытожив, итоговая модель показывает отличные результаты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdxuv_UQ1Tru"
      },
      "source": [
        "# 4. Имплементация алгоритма машинного обучения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suHUXcym4CNW"
      },
      "source": [
        "**a. Имплементация градиентного бустинга**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pvwkZDT3bnbu"
      },
      "outputs": [],
      "source": [
        "# a. Имплементация Gradient Boosting\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class DecisionTreeStump:\n",
        "\n",
        "\n",
        "    def __init__(self, max_depth=3):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Строим дерево рекурсивно\n",
        "        \"\"\"\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        Рекурсивное построение дерева\n",
        "        \"\"\"\n",
        "        n_samples = len(y)\n",
        "\n",
        "        # Условие остановки: достигли максимальной глубины или мало сэмплов\n",
        "        if depth >= self.max_depth or n_samples < 2:\n",
        "            return {'value': np.mean(y)}\n",
        "\n",
        "        # Ищем лучший split\n",
        "        best_gain = -1\n",
        "        best_split = None\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        for feature_idx in range(n_features):\n",
        "            values = X[:, feature_idx]\n",
        "            unique_values = np.unique(values)\n",
        "\n",
        "            # Пробуем разные пороги\n",
        "            for threshold in unique_values:\n",
        "                left_mask = values <= threshold\n",
        "                right_mask = values > threshold\n",
        "\n",
        "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Вычисляем gain (уменьшение MSE)\n",
        "                current_mse = np.var(y)\n",
        "                left_mse = np.var(y[left_mask])\n",
        "                right_mse = np.var(y[right_mask])\n",
        "\n",
        "                left_weight = np.sum(left_mask) / n_samples\n",
        "                right_weight = np.sum(right_mask) / n_samples\n",
        "\n",
        "                gain = current_mse - (left_weight * left_mse + right_weight * right_mse)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_split = {\n",
        "                        'feature_idx': feature_idx,\n",
        "                        'threshold': threshold,\n",
        "                        'left_mask': left_mask,\n",
        "                        'right_mask': right_mask\n",
        "                    }\n",
        "\n",
        "        # Если не нашли хороший split, возвращаем лист\n",
        "        if best_split is None or best_gain <= 0:\n",
        "            return {'value': np.mean(y)}\n",
        "\n",
        "        # Рекурсивно строим левое и правое поддеревья\n",
        "        left_tree = self._build_tree(X[best_split['left_mask']], y[best_split['left_mask']], depth + 1)\n",
        "        right_tree = self._build_tree(X[best_split['right_mask']], y[best_split['right_mask']], depth + 1)\n",
        "\n",
        "        return {\n",
        "            'feature_idx': best_split['feature_idx'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left': left_tree,\n",
        "            'right': right_tree\n",
        "        }\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Предсказания для массива X\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict_single(self, x, node):\n",
        "        \"\"\"\n",
        "        Предсказание для одного сэмпла\n",
        "        \"\"\"\n",
        "        # Если это лист, возвращаем значение\n",
        "        if 'value' in node:\n",
        "            return node['value']\n",
        "\n",
        "        # Идём влево или вправо в зависимости от порога\n",
        "        if x[node['feature_idx']] <= node['threshold']:\n",
        "            return self._predict_single(x, node['left'])\n",
        "        else:\n",
        "            return self._predict_single(x, node['right'])\n",
        "\n",
        "\n",
        "class GradientBoostingClassifierCustom:\n",
        "    \"\"\"\n",
        "    Имплементация градиентного бустинга для бинарной классификации\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Сигмоида для преобразования в вероятности\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение градиентного бустинга\n",
        "        \"\"\"\n",
        "        # Начальное предсказание - логарифм odds\n",
        "        pos_ratio = np.sum(y) / len(y)\n",
        "        self.initial_prediction = np.log(pos_ratio / (1 - pos_ratio + 1e-10))\n",
        "\n",
        "        # Текущие предсказания (в логит-пространстве)\n",
        "        current_predictions = np.full(len(y), self.initial_prediction)\n",
        "\n",
        "        # Строим деревья итеративно\n",
        "        for i in range(self.n_estimators):\n",
        "            # Вычисляем градиенты (остатки)\n",
        "            probabilities = self._sigmoid(current_predictions)\n",
        "            gradients = y - probabilities\n",
        "\n",
        "            # Обучаем дерево на градиентах\n",
        "            tree = DecisionTreeStump(max_depth=self.max_depth)\n",
        "            tree.fit(X, gradients)\n",
        "\n",
        "            # Делаем предсказания и обновляем текущие предсказания\n",
        "            tree_predictions = tree.predict(X)\n",
        "            current_predictions += self.learning_rate * tree_predictions\n",
        "\n",
        "            self.trees.append(tree)\n",
        "\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"Обучено деревьев: {i + 1}/{self.n_estimators}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Предсказание вероятностей\n",
        "        \"\"\"\n",
        "        # Начинаем с начального предсказания\n",
        "        predictions = np.full(len(X), self.initial_prediction)\n",
        "\n",
        "        # Добавляем предсказания всех деревьев\n",
        "        for tree in self.trees:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        # Преобразуем в вероятности через сигмоиду\n",
        "        probabilities = self._sigmoid(predictions)\n",
        "        return probabilities\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Предсказание классов\n",
        "        \"\"\"\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return (probabilities >= 0.5).astype(int)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkFlLkFT4WrT"
      },
      "source": [
        "**b. Обучение имплементированной модели с базовым бейзлайном**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDFEUh9Gb0K4",
        "outputId": "b47ad7d0-e429-4d6b-874d-00e8b9f0530f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обучение началось\n",
            "Обучено деревьев: 20/100\n",
            "Обучено деревьев: 40/100\n",
            "Обучено деревьев: 60/100\n",
            "Обучено деревьев: 80/100\n",
            "Обучено деревьев: 100/100\n",
            "\n",
            "Градиентный бустинг без улучшений обучен\n"
          ]
        }
      ],
      "source": [
        "# Загружаем датасет\n",
        "df_base = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Label Encoding\n",
        "for col in ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']:\n",
        "    le = LabelEncoder()\n",
        "    df_base[col] = le.fit_transform(df_base[col])\n",
        "\n",
        "# Заполняем пропуски\n",
        "df_base = df_base.fillna(df_base.median())\n",
        "\n",
        "# Разделяем\n",
        "X_base = df_base.drop('loan_status', axis=1)\n",
        "y_base = df_base['loan_status']\n",
        "\n",
        "X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base, y_base, test_size=0.2, random_state=42, stratify=y_base)\n",
        "\n",
        "# Нормализуем\n",
        "scaler_base = StandardScaler()\n",
        "X_train_base = scaler_base.fit_transform(X_train_base)\n",
        "X_test_base = scaler_base.transform(X_test_base)\n",
        "\n",
        "# Конвертируем в numpy\n",
        "X_train_base = np.array(X_train_base)\n",
        "X_test_base = np.array(X_test_base)\n",
        "y_train_base = np.array(y_train_base)\n",
        "y_test_base = np.array(y_test_base)\n",
        "\n",
        "# Используем имплементированный градиентный бустинг с дефолтными параметрами\n",
        "print(\"Обучение началось\")\n",
        "gb_base = GradientBoostingClassifierCustom(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "gb_base.fit(X_train_base, y_train_base)\n",
        "\n",
        "print(\"\\nГрадиентный бустинг без улучшений обучен\")\n",
        "\n",
        "y_pred_base = gb_base.predict(X_test_base)\n",
        "y_pred_proba_base = gb_base.predict_proba(X_test_base)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f8ufWAa4d4j"
      },
      "source": [
        "**c. Оценка качества имплементированной модели с базовым бейзлайном**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruzSpAT4fipT",
        "outputId": "45f4ac1f-8591-4553-da6c-22dc9cfdc9f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9023\n",
            "Precision: 0.9051\n",
            "Recall: 0.6167\n",
            "F1-Score: 0.7336\n",
            "ROC-AUC: 0.9024\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {accuracy_score(y_test_base, y_pred_base):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_base, y_pred_base):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test_base, y_pred_base):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test_base, y_pred_base):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test_base, y_pred_proba_base):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T-R-KbR4v5X"
      },
      "source": [
        "**d. Сравнение результатов с пунктом 2**\n",
        "\n",
        "\n",
        "Имплементированный градиентный бустинг показал результаты близкие к sklearn, но немного хуже:\n",
        "- Accuracy: 90.23% (sklearn: 92.22%)\n",
        "- Precision: 90.51% (sklearn: 94.12%)\n",
        "- Recall: 61.67% (sklearn: 68.64%)\n",
        "- F1-Score: 73.36% (sklearn: 79.38%)\n",
        "- ROC-AUC: 90.24% (sklearn: 92.54%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9stXO4M_43dI"
      },
      "source": [
        "**e. Выводы**\n",
        "\n",
        "Имплементированный градиентный бустинг работает корректно и показывает результаты близкие к sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2ziDbZp4-WI"
      },
      "source": [
        "**f. и g. Добавление техник из улучшенного бейзлайна и обучение имплементированной модели**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFm6Z3RJgJWg",
        "outputId": "17fc956f-ca91-4620-9f00-aba94dc751b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обучение началось\n",
            "Параметры: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5}\n",
            "Обучено деревьев: 20/200\n",
            "Обучено деревьев: 40/200\n",
            "Обучено деревьев: 60/200\n",
            "Обучено деревьев: 80/200\n",
            "Обучено деревьев: 100/200\n",
            "Обучено деревьев: 120/200\n",
            "Обучено деревьев: 140/200\n",
            "Обучено деревьев: 160/200\n",
            "Обучено деревьев: 180/200\n",
            "Обучено деревьев: 200/200\n",
            "\n",
            "Градиентный бустинг с улучшениями обучен\n",
            "\n",
            "Размер тестового набора: 6336\n",
            "Первые 10 предсказаний: [0 0 0 0 0 0 0 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "# Загружаем датасет\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Удаляем выбросы\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "\n",
        "# Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Конвертируем в numpy\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Обучаем с лучшими параметрами из гипотезы 2\n",
        "print(\"Обучение началось\")\n",
        "print(f\"Параметры: {best_params}\")\n",
        "\n",
        "gb_custom = GradientBoostingClassifierCustom(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    max_depth=best_params['max_depth']\n",
        ")\n",
        "gb_custom.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nГрадиентный бустинг с улучшениями обучен\")\n",
        "\n",
        "# Предсказания\n",
        "y_pred_custom = gb_custom.predict(X_test)\n",
        "y_pred_proba_custom = gb_custom.predict_proba(X_test)\n",
        "\n",
        "print(f\"\\nРазмер тестового набора: {len(X_test)}\")\n",
        "print(f\"Первые 10 предсказаний: {y_pred_custom[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVs95V1F5L3i"
      },
      "source": [
        "**h. Оценка качества имплементированной моедли с улучшенным бейзлайном**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2hlFANvqjAc",
        "outputId": "cabce10b-13bb-4aab-ac21-d261f4c518a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9334\n",
            "Precision: 0.9886\n",
            "Recall: 0.6989\n",
            "F1-Score: 0.8189\n",
            "ROC-AUC: 0.9302\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_custom):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_custom):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_custom):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_custom):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_custom):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcfuPZEG5UOh"
      },
      "source": [
        "**i. Сравнение результатов с пунктом 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGqLvvsbqs19"
      },
      "source": [
        "Имплементированный градиентный бустинг с улучшениями показал результаты близкие к sklearn, но немного хуже. Accuracy составила 93.34% против 94.02% у sklearn. Precision оказалась даже выше: 98.86% против 96.95%, Recall немного ниже: 69.89% против 74.58%, то есть модель пропускает на 5% больше дефолтов. F1-Score составил 81.89% против 84.31% у sklearn. ROC-AUC показал 93.02% против 95.24%. В целом, результаты близки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4SbGWNh5Yyk"
      },
      "source": [
        "**j. Выводы**\n",
        "\n",
        "Моя версия градиентного бустинга показала себя неплохо, результаты похожи,особенно мне понравился результат Precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7-rTKkQzcJ8"
      },
      "source": [
        "## Регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNlSTxXU1qrZ"
      },
      "source": [
        "# 2. Создание бейзлайна и оценка качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEcaBanA2q33"
      },
      "source": [
        "**a. Обучение модели sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtauCU69kBLD",
        "outputId": "076b2775-9ddf-46e7-8be2-0ab23b201451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Градиентный бустинг Бейзлайн\n",
            "MAE: 403285.50\n",
            "RMSE: 508714.56\n",
            "R²: 0.5889\n",
            "MAPE: 39.42%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['Company_Name', 'Job_Role', 'Experience_Level', 'City']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Заполняем пропуски медианой\n",
        "df = df.fillna(df.median())\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучение\n",
        "gb = GradientBoostingRegressor(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Вычисляем метрики\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "# Вывод метрик\n",
        "print(\"Градиентный бустинг Бейзлайн\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGN4Req-2q33"
      },
      "source": [
        "**b. Оценка качества модели**\n",
        "\n",
        "Градиентный бустинг на базовом бейзлайне показывает средние результаты, необходимы улучшения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV5ObDig1spe"
      },
      "source": [
        "# 3. Улучшение бейзлайна"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBrfxm3C37xx"
      },
      "source": [
        "**a. Формулирование гипотез**\n",
        "\n",
        "Удалить выбросы в зарплатах (слишком высокие или низкие значения искажают модель), использовать One-hot encoding вместо Label Encoding для категориальных признаков (Company_Name, Job_Role, City, Experience_Level), чтобы модель корректнее работала с категориями.\n",
        "\n",
        "**Гипотеза 2: Подбор гиперпараметров**\n",
        "\n",
        "Подобрать оптимальные параметры: n_estimators, learning_rate, max_depth и min_samples_split с помощью кросс-валидации. Проверить разные комбинации параметров.\n",
        "\n",
        "**Гипотеза 3: Новые признаки**\n",
        "\n",
        "Создать новые признаки: средняя зарплата по городу, средняя зарплата по должности, средняя зарплата по уровню опыта. Это должно помочь модели лучше понимать рыночные тренды.\n",
        "\n",
        "**Гипотеза 4: Subsample**\n",
        "\n",
        "Использовать параметр subsample (например, 0.8), чтобы на каждой итерации использовалась случайная подвыборка 80% данных. Это должно уменьшить переобучение, ускорить обучение и улучшить обобщающую способность модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFeFrv8Y37xx"
      },
      "source": [
        "**b. Проверка гипотез**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moW8ly-GlJQB",
        "outputId": "422f54b3-9c12-4350-fcf5-6846fb0c44a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "До удаления выбросов: 30000\n",
            "После удаления выбросов: 29400\n",
            "Количество признаков после One-hot: 66\n",
            "Гипотеза 1: препроцессинг\n",
            "MAE: 391588.07\n",
            "RMSE: 490045.11\n",
            "R²: 0.5426\n",
            "MAPE: 39.34%\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 1: препроцессинг (удаление выбросов + One-hot encoding)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Удаляем выбросы в зарплате\n",
        "print(\"До удаления выбросов:\", len(df))\n",
        "q1 = df['Salary_INR'].quantile(0.01)\n",
        "q99 = df['Salary_INR'].quantile(0.99)\n",
        "df = df[(df['Salary_INR'] >= q1) & (df['Salary_INR'] <= q99)]\n",
        "print(\"После удаления выбросов:\", len(df))\n",
        "\n",
        "# Заполняем пропуски только в числовых колонках\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# One-hot encoding для категориальных признаков\n",
        "df = pd.get_dummies(df, columns=['Company_Name', 'Job_Role', 'Experience_Level', 'City'], drop_first=False)\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "print(f\"Количество признаков после One-hot: {X.shape[1]}\")\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем\n",
        "gb = GradientBoostingRegressor(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Метрики\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "print(\"Гипотеза 1: препроцессинг\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4WoOxFjlXjd"
      },
      "source": [
        "**Вывод по Гипотезе 1:**\n",
        "\n",
        "Результаты неоднозначные\n",
        "\n",
        "R² немного упал с 0.5889 до 0.5426\n",
        "\n",
        "MAE и RMSE немного улучшились\n",
        "\n",
        "MAPE практически не изменился (39.34% vs 39.42%)\n",
        "\n",
        "Препроцессинг дал небольшое улучшение по абсолютным ошибкам, но общее качество модели слегка снизилось. Гипотеза 1 частично подтверждена, но эффект не такой сильный, как ожидалось.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azITrBZLla_n",
        "outputId": "8bb36219-a595-46f7-a2c2-fd8b4a5d91b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Проверка разных комбинаций параметров с кросс-валидацией (3-fold):\n",
            "\n",
            "n_est=100, lr=0.05, depth=3: R² (3-fold CV) = 0.5364\n",
            "n_est=150, lr=0.1, depth=3: R² (3-fold CV) = 0.5490\n",
            "n_est=200, lr=0.1, depth=5: R² (3-fold CV) = 0.5327\n",
            "n_est=150, lr=0.05, depth=5: R² (3-fold CV) = 0.5429\n",
            "n_est=200, lr=0.05, depth=4: R² (3-fold CV) = 0.5460\n",
            "\n",
            "Лучшие параметры: {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 3} (R² = 0.5490)\n",
            "\n",
            "Гипотеза 2: подбор гиперпараметров (n_est=150, lr=0.1, depth=3)\n",
            "MAE: 391680.45\n",
            "RMSE: 490267.13\n",
            "R²: 0.5421\n",
            "MAPE: 39.26%\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 2: подбор гиперпараметров\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Проверяем разные комбинации параметров\n",
        "param_grid = [\n",
        "    {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 3},\n",
        "    {'n_estimators': 150, 'learning_rate': 0.1, 'max_depth': 3},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5},\n",
        "    {'n_estimators': 150, 'learning_rate': 0.05, 'max_depth': 5},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 4},\n",
        "]\n",
        "\n",
        "best_params = None\n",
        "best_r2 = -float('inf')\n",
        "\n",
        "print(\"Проверка разных комбинаций параметров с кросс-валидацией (3-fold):\\n\")\n",
        "\n",
        "for params in param_grid:\n",
        "    gb = GradientBoostingRegressor(\n",
        "        n_estimators=params['n_estimators'],\n",
        "        learning_rate=params['learning_rate'],\n",
        "        max_depth=params['max_depth'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Кросс-валидация по R²\n",
        "    cv_scores = cross_val_score(gb, X_train, y_train, cv=3, scoring='r2')\n",
        "    mean_r2 = cv_scores.mean()\n",
        "\n",
        "    print(f\"n_est={params['n_estimators']}, lr={params['learning_rate']}, depth={params['max_depth']}: R² (3-fold CV) = {mean_r2:.4f}\")\n",
        "\n",
        "    if mean_r2 > best_r2:\n",
        "        best_r2 = mean_r2\n",
        "        best_params = params\n",
        "\n",
        "print(f\"\\nЛучшие параметры: {best_params} (R² = {best_r2:.4f})\")\n",
        "\n",
        "# Обучаем с лучшими параметрами\n",
        "gb_best = GradientBoostingRegressor(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    learning_rate=best_params['learning_rate'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    random_state=42\n",
        ")\n",
        "gb_best.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb_best.predict(X_test)\n",
        "\n",
        "print(f\"\\nГипотеза 2: подбор гиперпараметров (n_est={best_params['n_estimators']}, lr={best_params['learning_rate']}, depth={best_params['max_depth']})\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GlSnSwZmNnr"
      },
      "source": [
        "**Вывод по Гипотезе 2:**\n",
        "\n",
        "Подбор гиперпараметров дал небольшое улучшение\n",
        "\n",
        "Лучшая комбинация: n_estimators=150, learning_rate=0.1, max_depth=3\n",
        "\n",
        "R² остался примерно на том же уровне (0.5421)\n",
        "\n",
        "MAE и RMSE практически не изменились\n",
        "\n",
        "MAPE немного улучшился (39.26% vs 39.34%)\n",
        "\n",
        "Гипотеза 2 подтверждена частично, увеличение количества деревьев до 150 дало небольшой прирост качества."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMmgGVpamRSd",
        "outputId": "134c89aa-8036-422c-fc72-1a6fee7b81ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Новые признаки созданы:\n",
            " - avg_salary_by_city: min=1255915.13, max=1296443.51\n",
            " - avg_salary_by_job: min=765232.42, max=2584490.48\n",
            " - avg_salary_by_exp: min=1251935.92, max=1283878.06\n",
            "\n",
            "Количество признаков (без новых): 66\n",
            "Количество признаков (с новыми): 69\n",
            "\n",
            "Гипотеза 3: новые признаки (n_est=150, lr=0.1, depth=3)\n",
            "MAE: 391967.84\n",
            "RMSE: 490672.56\n",
            "R²: 0.5414\n",
            "MAPE: 39.19%\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 3: новые признаки\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Удаляем выбросы\n",
        "q1 = df['Salary_INR'].quantile(0.01)\n",
        "q99 = df['Salary_INR'].quantile(0.99)\n",
        "df = df[(df['Salary_INR'] >= q1) & (df['Salary_INR'] <= q99)]\n",
        "\n",
        "# Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Создаём новые признаки до One-hot encoding\n",
        "df['avg_salary_by_city'] = df.groupby('City')['Salary_INR'].transform('mean')\n",
        "df['avg_salary_by_job'] = df.groupby('Job_Role')['Salary_INR'].transform('mean')\n",
        "df['avg_salary_by_exp'] = df.groupby('Experience_Level')['Salary_INR'].transform('mean')\n",
        "\n",
        "print(\"Новые признаки созданы:\")\n",
        "print(f\" - avg_salary_by_city: min={df['avg_salary_by_city'].min():.2f}, max={df['avg_salary_by_city'].max():.2f}\")\n",
        "print(f\" - avg_salary_by_job: min={df['avg_salary_by_job'].min():.2f}, max={df['avg_salary_by_job'].max():.2f}\")\n",
        "print(f\" - avg_salary_by_exp: min={df['avg_salary_by_exp'].min():.2f}, max={df['avg_salary_by_exp'].max():.2f}\")\n",
        "\n",
        "# One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Company_Name', 'Job_Role', 'Experience_Level', 'City'], drop_first=False)\n",
        "\n",
        "# Разделяем\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "print(f\"\\nКоличество признаков (без новых): 66\")\n",
        "print(f\"Количество признаков (с новыми): {X.shape[1]}\")\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализуем\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем с лучшими параметрами из гипотезы 2\n",
        "gb_fe = GradientBoostingRegressor(n_estimators=150, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_fe.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb_fe.predict(X_test)\n",
        "\n",
        "print(\"\\nГипотеза 3: новые признаки (n_est=150, lr=0.1, depth=3)\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Pt8up6mgC8"
      },
      "source": [
        "**Вывод по гипотезе 3:**\n",
        "\n",
        "Новые признаки не улучшили качество\n",
        "\n",
        "R² остался на том же уровне (0.5414)\n",
        "\n",
        "MAE и RMSE практически не изменились\n",
        "\n",
        "MAPE немного улучшился (39.19%)\n",
        "\n",
        "Средние зарплаты по городу, должности и опыту не добавили модели новой полезной информации, так как градиенный бустинг и так хорошо улавливает такие закономерности через категориальные признаки.\n",
        "\n",
        "Гипотеза 3 не подтверждена, игнорируется."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBVZ6TYMmnDf",
        "outputId": "874c59eb-3c26-4b58-a9c6-e47634d735aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Проверка разных значений subsample:\n",
            "\n",
            "subsample=0.6: R² = 0.5412\n",
            "subsample=0.7: R² = 0.5410\n",
            "subsample=0.8: R² = 0.5418\n",
            "subsample=0.9: R² = 0.5417\n",
            "subsample=1.0: R² = 0.5421\n",
            "\n",
            "Лучший subsample = 1.0 (R² = 0.5421)\n",
            "\n",
            "Гипотеза 4: subsample=1.0 (n_est=150, lr=0.1, depth=3)\n",
            "MAE: 391680.45\n",
            "RMSE: 490267.13\n",
            "R²: 0.5421\n",
            "MAPE: 39.26%\n"
          ]
        }
      ],
      "source": [
        "# Гипотеза 4: Subsample\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Удаляем выбросы\n",
        "q1 = df['Salary_INR'].quantile(0.01)\n",
        "q99 = df['Salary_INR'].quantile(0.99)\n",
        "df = df[(df['Salary_INR'] >= q1) & (df['Salary_INR'] <= q99)]\n",
        "\n",
        "# Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Company_Name', 'Job_Role', 'Experience_Level', 'City'], drop_first=False)\n",
        "\n",
        "# Разделяем\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализуем\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Тестируем разные значения subsample\n",
        "subsample_values = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "print(\"Проверка разных значений subsample:\\n\")\n",
        "\n",
        "best_subsample = None\n",
        "best_r2 = -float('inf')\n",
        "\n",
        "for sub in subsample_values:\n",
        "    gb_sub = GradientBoostingRegressor(\n",
        "        n_estimators=150,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        subsample=sub,\n",
        "        random_state=42\n",
        "    )\n",
        "    gb_sub.fit(X_train, y_train)\n",
        "    y_pred_sub = gb_sub.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred_sub)\n",
        "\n",
        "    print(f\"subsample={sub}: R² = {r2:.4f}\")\n",
        "\n",
        "    if r2 > best_r2:\n",
        "        best_r2 = r2\n",
        "        best_subsample = sub\n",
        "\n",
        "print(f\"\\nЛучший subsample = {best_subsample} (R² = {best_r2:.4f})\")\n",
        "\n",
        "# Обучаем с лучшим subsample\n",
        "gb_subsample = GradientBoostingRegressor(\n",
        "    n_estimators=150,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    subsample=best_subsample,\n",
        "    random_state=42\n",
        ")\n",
        "gb_subsample.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb_subsample.predict(X_test)\n",
        "\n",
        "print(f\"\\nГипотеза 4: subsample={best_subsample} (n_est=150, lr=0.1, depth=3)\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5v57Qcfmlha"
      },
      "source": [
        "**Вывод по Гипотезе 4:**\n",
        "\n",
        "Увеличение количества деревьев не улучшило качество\n",
        "\n",
        "R² даже немного упал (0.5407)\n",
        "\n",
        "MAE и RMSE остались на том же уровне\n",
        "\n",
        "MAPE практически не изменился\n",
        "\n",
        "Модель с 250 деревьями переобучается или достигла потолка качества.\n",
        "\n",
        "Гипотеза 4 не подтверждена, игнорируется.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmhI2efl37xx"
      },
      "source": [
        "**c., d. и e. Формирование улучшенного бейзлайна, обучение модели и оценка качсетва моедли**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ_6Z-aTm8Yt",
        "outputId": "e4809a84-5013-46f3-85f9-ba56c803902f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Улучшенный бейзлайн\n",
            "MAE: 391680.45\n",
            "RMSE: 490267.13\n",
            "R²: 0.5421\n",
            "MAPE: 39.26%\n"
          ]
        }
      ],
      "source": [
        "# Улучшенный бейзлайн\n",
        "# Гипотеза 1 (Препроцессинг) + гипотеза 2 (n_estimators=150, lr=0.1, depth=3)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Улучшение 1: Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Улучшение 2: Удаляем выбросы\n",
        "q1 = df['Salary_INR'].quantile(0.01)\n",
        "q99 = df['Salary_INR'].quantile(0.99)\n",
        "df = df[(df['Salary_INR'] >= q1) & (df['Salary_INR'] <= q99)]\n",
        "\n",
        "# Улучшение 3: Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Улучшение 4: One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Company_Name', 'Job_Role', 'Experience_Level', 'City'], drop_first=False)\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Улучшение 5: Оптимальные гиперпараметры (n_estimators=150, learning_rate=0.1, max_depth=3)\n",
        "gb_improved = GradientBoostingRegressor(n_estimators=150, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_improved.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = gb_improved.predict(X_test)\n",
        "\n",
        "# Метрики\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "print(\"Улучшенный бейзлайн\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUozSPQB37xy"
      },
      "source": [
        "**f. Сравнение результатов с пунктом 2**\n",
        "\n",
        "Сравнение базового и улучшенного бейзлайна:\n",
        "\n",
        "Базовый бейзлайн:\n",
        "- MAE: 403285.50\n",
        "- RMSE: 508714.56\n",
        "- R²: 0.5889\n",
        "- MAPE: 39.42%\n",
        "\n",
        "Улучшенный бейзлайн:\n",
        "- MAE: 391680.45\n",
        "- RMSE: 490267.13\n",
        "- R²: 0.5421\n",
        "- MAPE: 39.26%\n",
        "\n",
        "Абсолютные ошибки стали меньше, но R² упал. Это связано с удалением выбросов - модель стала точнее предсказывать типичные зарплаты, но общая объясняющая способность снизилась из-за уменьшения дисперсии целевой переменной."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKR9UuY437xy"
      },
      "source": [
        "**g. Выводы**\n",
        "\n",
        "Улучшенный бейзлайн показал смешанные результаты. MAE и RMSE стали меньше, значит модель стала точнее в самих суммах. R² упал, но это нормально, потому что после удаления выбросов разброс зарплат стал меньше.\n",
        "\n",
        "Из четырёх идей сработали две: удаление выбросов и One-hot encoding, а также увеличение количества деревьев до 150. Новые фичи и параметр subsample почти ничего не дали.\n",
        "\n",
        "В итоге улучшенный бейзлайн всё равно выглядит лучше, потому что в прогнозе зарплат важнее маленькая ошибка в рупиях, чем высокий R².\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZcP8GE_1wVv"
      },
      "source": [
        "# 4. Имплементация алгоритма машинного обучения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxfc3ek25f0_"
      },
      "source": [
        "**a. Имплементация градиентного бустинга**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ihdHNrlaFxEk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class DecisionTreeStump:\n",
        "\n",
        "    def __init__(self, max_depth=3):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Строим дерево рекурсивно\n",
        "        \"\"\"\n",
        "        self.tree = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        Рекурсивное построение дерева\n",
        "        \"\"\"\n",
        "        n_samples = len(y)\n",
        "\n",
        "        # Условие остановки: достигли максимальной глубины или мало сэмплов\n",
        "        if depth >= self.max_depth or n_samples < 2:\n",
        "            return {'value': np.mean(y)}\n",
        "\n",
        "        # Ищем лучший split\n",
        "        best_gain = -1\n",
        "        best_split = None\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        for feature_idx in range(n_features):\n",
        "            values = X[:, feature_idx]\n",
        "            unique_values = np.unique(values)\n",
        "\n",
        "            # Пробуем разные пороги\n",
        "            for threshold in unique_values:\n",
        "                left_mask = values <= threshold\n",
        "                right_mask = values > threshold\n",
        "\n",
        "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Вычисляем gain (уменьшение MSE)\n",
        "                current_mse = np.var(y)\n",
        "                left_mse = np.var(y[left_mask])\n",
        "                right_mse = np.var(y[right_mask])\n",
        "\n",
        "                left_weight = np.sum(left_mask) / n_samples\n",
        "                right_weight = np.sum(right_mask) / n_samples\n",
        "\n",
        "                gain = current_mse - (left_weight * left_mse + right_weight * right_mse)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_split = {\n",
        "                        'feature_idx': feature_idx,\n",
        "                        'threshold': threshold,\n",
        "                        'left_mask': left_mask,\n",
        "                        'right_mask': right_mask\n",
        "                    }\n",
        "\n",
        "        # Если не нашли хороший split, возвращаем лист\n",
        "        if best_split is None or best_gain <= 0:\n",
        "            return {'value': np.mean(y)}\n",
        "\n",
        "        # Рекурсивно строим левое и правое поддеревья\n",
        "        left_tree = self._build_tree(X[best_split['left_mask']], y[best_split['left_mask']], depth + 1)\n",
        "        right_tree = self._build_tree(X[best_split['right_mask']], y[best_split['right_mask']], depth + 1)\n",
        "\n",
        "        return {\n",
        "            'feature_idx': best_split['feature_idx'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left': left_tree,\n",
        "            'right': right_tree\n",
        "        }\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Предсказания для массива X\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
        "\n",
        "    def _predict_single(self, x, node):\n",
        "        \"\"\"\n",
        "        Предсказание для одного сэмпла\n",
        "        \"\"\"\n",
        "        # Если это лист, возвращаем значение\n",
        "        if 'value' in node:\n",
        "            return node['value']\n",
        "\n",
        "        # Идём влево или вправо в зависимости от порога\n",
        "        if x[node['feature_idx']] <= node['threshold']:\n",
        "            return self._predict_single(x, node['left'])\n",
        "        else:\n",
        "            return self._predict_single(x, node['right'])\n",
        "\n",
        "\n",
        "\n",
        "class GradientBoostingRegressorCustom:\n",
        "\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.initial_prediction = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Обучение\n",
        "        \"\"\"\n",
        "        # Начальное предсказание - среднее значение таргета\n",
        "        self.initial_prediction = np.mean(y)\n",
        "\n",
        "        # Текущие предсказания\n",
        "        current_predictions = np.full(len(y), self.initial_prediction)\n",
        "\n",
        "        # Строим деревья итеративно\n",
        "        for i in range(self.n_estimators):\n",
        "            # Вычисляем градиенты\n",
        "            gradients = y - current_predictions\n",
        "\n",
        "            # Обучаем дерево на градиентах\n",
        "            tree = DecisionTreeStump(max_depth=self.max_depth)\n",
        "            tree.fit(X, gradients)\n",
        "\n",
        "            # Делаем предсказания и обновляем текущие предсказания\n",
        "            tree_predictions = tree.predict(X)\n",
        "            current_predictions += self.learning_rate * tree_predictions\n",
        "\n",
        "            self.trees.append(tree)\n",
        "\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"Обучено деревьев: {i + 1}/{self.n_estimators}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Предсказание значений\n",
        "        \"\"\"\n",
        "        # Начинаем с начального предсказания\n",
        "        predictions = np.full(len(X), self.initial_prediction)\n",
        "\n",
        "        # Добавляем предсказания всех деревьев\n",
        "        for tree in self.trees:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1gvre-Q5f1A"
      },
      "source": [
        "**b. Обучение имплементированной модели с базовым бейзлайном**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDkTaIZ5GJK5",
        "outputId": "351519b6-1e36-41b8-940b-6b9bb6533b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обучено деревьев: 20/100\n",
            "Обучено деревьев: 40/100\n",
            "Обучено деревьев: 60/100\n",
            "Обучено деревьев: 80/100\n",
            "Обучено деревьев: 100/100\n",
            "\n",
            "Градиентный бустинг обучен\n",
            "Размер тестового набора: 6000\n",
            "Первые 10 предсказаний: [1482389.9975223   932868.98765693  768987.76921463  939701.02173936\n",
            "  909708.24560555  773631.40264392 1124257.7427483   778507.16623275\n",
            "  761368.66433412  928396.6828369 ]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['Company_Name', 'Job_Role', 'Experience_Level', 'City']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Заполняем пропуски медианой\n",
        "df = df.fillna(df.median())\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Конвертируем в numpy\n",
        "X_train_np = np.array(X_train_scaled)\n",
        "X_test_np = np.array(X_test_scaled)\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "# Обучаем\n",
        "gb_custom = GradientBoostingRegressorCustom(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "gb_custom.fit(X_train_np, y_train_np)\n",
        "\n",
        "print(\"\\nГрадиентный бустинг обучен\")\n",
        "\n",
        "# Предсказания\n",
        "y_pred_custom = gb_custom.predict(X_test_np)\n",
        "\n",
        "print(f\"Размер тестового набора: {len(X_test_np)}\")\n",
        "print(f\"Первые 10 предсказаний: {y_pred_custom[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmBN7ODG5f1A"
      },
      "source": [
        "**c. Оценка качества имплементированной модели с базовым бейзлайном**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22l37eMLGy4a",
        "outputId": "fc7ab68c-60be-4583-b4ea-04f2a9f10c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE: 403285.50\n",
            "RMSE: 508714.56\n",
            "R²: 0.5889\n",
            "MAPE: 39.42%\n"
          ]
        }
      ],
      "source": [
        "# Метрики\n",
        "mae = mean_absolute_error(y_test_np, y_pred_custom)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_custom))\n",
        "r2 = r2_score(y_test_np, y_pred_custom)\n",
        "mape = np.mean(np.abs((y_test_np - y_pred_custom) / y_test_np)) * 100\n",
        "\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43EHS5335f1A"
      },
      "source": [
        "**d. Сравнение результатов с пунктом 2**\n",
        "\n",
        "**sklearn:**\n",
        "\n",
        "MAE: 403285.50\n",
        "\n",
        "RMSE: 508714.56\n",
        "\n",
        "R²: 0.5889\n",
        "\n",
        "MAPE: 39.42%\n",
        "\n",
        "**Имплементированынй:**\n",
        "\n",
        "MAE: 403285.50\n",
        "\n",
        "RMSE: 508714.56\n",
        "\n",
        "R²: 0.5889\n",
        "\n",
        "MAPE: 39.42%\n",
        "\n",
        "Это подтверждает правильность реализации алгоритма градиентного бустинга."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W4RjZfV5f1A"
      },
      "source": [
        "**e. Выводы**\n",
        "\n",
        "Результаты полностью сошлись со вторым пунктом, это говорит о правильности имплементации алгоритма градиентного бустинга. Модель последовательно строит деревья, каждое из которых обучается на остатках предыдущих, и суммирует их предсказания с учётом learning_rate. Время выполнения было заметно больше, чем в случае sklearn, но метрики идентичны, что подтверждает корректность работы всех компонентов: построения деревьев, вычисления градиентов и итеративного обучения.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzFXsXXk5f1A"
      },
      "source": [
        "**f. Добавление техник из улучшенного бейзлайна и обучение имплементированной модели**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlXlf7VQHLnC",
        "outputId": "7969647a-0e92-4d2c-e8d1-8deac0fd76b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обучено деревьев: 20/150\n",
            "Обучено деревьев: 40/150\n",
            "Обучено деревьев: 60/150\n",
            "Обучено деревьев: 80/150\n",
            "Обучено деревьев: 100/150\n",
            "Обучено деревьев: 120/150\n",
            "Обучено деревьев: 140/150\n",
            "\n",
            "Градиентный бустинг с улучшениями обучен\n",
            "Размер тестового набора: 5880\n",
            "Первые 10 предсказаний: [1022635.83107262  759936.21923362 1351621.89951146  953886.17672822\n",
            " 2447725.38077048 1113515.52576075 2448126.18076615 2794184.29777391\n",
            "  991823.87467645 1511045.00926519]\n"
          ]
        }
      ],
      "source": [
        "# Загружаем датасет\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Удаляем выбросы\n",
        "q1 = df['Salary_INR'].quantile(0.01)\n",
        "q99 = df['Salary_INR'].quantile(0.99)\n",
        "df = df[(df['Salary_INR'] >= q1) & (df['Salary_INR'] <= q99)]\n",
        "\n",
        "# Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Company_Name', 'Job_Role', 'Experience_Level', 'City'], drop_first=False)\n",
        "\n",
        "# Разделяем\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализуем\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Конвертируем в numpy\n",
        "X_train_np = np.array(X_train_scaled)\n",
        "X_test_np = np.array(X_test_scaled)\n",
        "y_train_np = np.array(y_train)\n",
        "y_test_np = np.array(y_test)\n",
        "\n",
        "# Обучаем с улучшенными параметрами (n_estimators=150)\n",
        "gb_custom_improved = GradientBoostingRegressorCustom(n_estimators=150, learning_rate=0.1, max_depth=3)\n",
        "gb_custom_improved.fit(X_train_np, y_train_np)\n",
        "\n",
        "print(\"\\nГрадиентный бустинг с улучшениями обучен\")\n",
        "\n",
        "# Предсказания\n",
        "y_pred_custom_improved = gb_custom_improved.predict(X_test_np)\n",
        "\n",
        "print(f\"Размер тестового набора: {len(X_test_np)}\")\n",
        "print(f\"Первые 10 предсказаний: {y_pred_custom_improved[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1AlOGEA5f1A"
      },
      "source": [
        "**h. Оценка качества имплементированной моедли с улучшенным бейзлайном**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAzhFcqSH4Hy",
        "outputId": "d972eb6d-12aa-4534-8f03-f6b2570e719d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Имплементированный градиентный бустинг с улучшениями (n_est=150, lr=0.1, depth=3)\n",
            "MAE: 391680.45\n",
            "RMSE: 490267.13\n",
            "R²: 0.5421\n",
            "MAPE: 39.26%\n"
          ]
        }
      ],
      "source": [
        "# Метрики\n",
        "mae = mean_absolute_error(y_test_np, y_pred_custom_improved)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_custom_improved))\n",
        "r2 = r2_score(y_test_np, y_pred_custom_improved)\n",
        "mape = np.mean(np.abs((y_test_np - y_pred_custom_improved) / y_test_np)) * 100\n",
        "\n",
        "print(\"Имплементированный градиентный бустинг с улучшениями (n_est=150, lr=0.1, depth=3)\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8a7NWhc5f1A"
      },
      "source": [
        "**i. Сравнение результатов с пунктом 3**\n",
        "\n",
        "Результаты полностью совпали, имплементированный градиентный бустинг с улучшениями дает точно такие же метрики, как sklearn:\n",
        "- MAE: 391680.45\n",
        "- RMSE: 490267.13\n",
        "- R²: 0.5421\n",
        "- MAPE: 39.26%\n",
        "\n",
        "Это подтверждает правильность реализации алгоритма и корректность применения всех улучшений из бейзлайна."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbX21Mdt5f1A"
      },
      "source": [
        "**j. Выводы**\n",
        "\n",
        "Результаты полностью сошлись с третьим пунктом, что доказывает правильность имплементации градиентного бустинга. Модель с улучшениями (препроцессинг + 150 деревьев) показала те же метрики, что и sklearn версия. Это означает, что алгоритм работает корректно: правильно вычисляются градиенты (остатки), деревья строятся на основе этих градиентов, а предсказания суммируются с учётом learning_rate.\n",
        "\n",
        "Имплементированная модель работает медленнее sklearn из-за отсутствия оптимизаций (например, sklearn использует Cython и эффективные структуры данных), но дает идентичные результаты, что подтверждает понимание принципов работы градиентного бустинга."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Итоговый вывод по 5 лабораторным\n",
        "\n",
        "За 5 лабораторных работ я прошелся по разным моделям - линейная и логистическая регрессия, KNN, решающее дерево, случайный лес и градиентный бустинг. Все модели делал и через sklearn, и писал сам с нуля. Тестил на двух задачах: классификация кредитного риска и регрессия зарплаты по рынку труда. Подбор параметров (перебор по сетке, кросс-валидация) почти везде давал заметный буст по метрикам. Свои реализации работали норм, но обычно чуть слабее sklearn, так как там больше оптимизаций.\n",
        "\n",
        "Классификация (топ-3 по ROC-AUC)\n",
        "| Место | Алгоритм            | F1-Score | ROC-AUC |\n",
        "| ----- | ------------------- | -------- | ------- |\n",
        "| 1     | Градиентный бустинг | 0.8431   | 0.9524  |\n",
        "| 2     | Случайный лес       | 0.8347   | 0.9354  |\n",
        "| 3     | Дерево решений      | 0.8277   | 0.9146  |\n",
        "\n",
        "Регрессия (топ-3 по R²)\n",
        "| Место | Алгоритм                       | R²     | MAPE   |\n",
        "| ----- | ------------------------------ | ------ | ------ |\n",
        "| 1     | Градиентный бустинг            | 0.5889 | 39.42% |\n",
        "| 2     | Случайный лес                  | 0.5435 | 40.37% |\n",
        "| 3     | Улучшенный градиентный бустинг | 0.5421 | 39.26% |\n",
        "\n",
        "- **KNN:** неплох, но капризный, сильно зависит от нормальных фичей и масштабирования; в классификации по Recall заметно слабее нижеупомянутых алгоритмов.\n",
        "\n",
        "- **логрег/линрег:** логистическая регрессия в классификации ведёт себя неплохо, а линейная регрессия в регрессии раскрывается только после One‑hot (иначе почти ноль по R²).\n",
        "\n",
        "- **деревья:** дерево быстро даёт прирост, но нужно ограничивать глубину/листья, иначе происходит переобучение.\n",
        "\n",
        "- **Случайный лес и бустинг:** чаще всего лучшие по итоговым метрикам, особенно после настройки параметров."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
