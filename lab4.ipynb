{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Лабораторная работа №4 (Проведение исследований со случайным лесом)"
      ],
      "metadata": {
        "id": "TwSlIIKkL6Te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Классификация"
      ],
      "metadata": {
        "id": "LmLUPmRlzZBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Создание бейзлайна и оценка качества"
      ],
      "metadata": {
        "id": "GVuBRo3dzfPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Обучение модели sklearn**"
      ],
      "metadata": {
        "id": "JXks6TXI2BiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Заполняем пропуски медианой\n",
        "df = df.fillna(df.median())\n",
        "\n",
        "# Разделяем на признаки и целевой класс\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучение Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf.predict(X_test)\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Случайный лес (n_estimators=100) Бейзлайн\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yXYZIjE9hwT",
        "outputId": "a9396c20-c7fc-4ab1-866b-a309bae4e74c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Случайный лес (n_estimators=100) Бейзлайн\n",
            "Accuracy: 0.9319\n",
            "Precision: 0.9693\n",
            "Recall: 0.7103\n",
            "F1-Score: 0.8198\n",
            "ROC-AUC: 0.9289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Оценка качества модели**\n",
        "\n",
        "- Accuracy: 0.9319\n",
        "- Precision: 0.9693\n",
        "- Recall: 0.7103\n",
        "- F1-Score: 0.8198\n",
        "- ROC-AUC: 0.9289\n",
        "\n",
        "\n",
        "Случайный лес показал значительно лучше результаты особенно по сравнению с KNN. Разделение классов в 93% это достойный результат, но есть потенциал для recall, чтобы ловить ещё больше дефолтов."
      ],
      "metadata": {
        "id": "D-Eyq04b2MHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Улучшение бейзлайна"
      ],
      "metadata": {
        "id": "AjPhND9m1HNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Формулировка гипотез**\n",
        "\n",
        "**Гипотеза 1: Препроцессинг**\n",
        "\n",
        "Удалить явные выбросы по возрасту и стажу (person_age < 100, person_emp_length < 100), заполнить пропуски в числовых колонках медианой и перейти с Label Encoding на One-hot для категориальных признаков (person_home_ownership, loan_intent, loan_grade, cb_person_default_on_file), чтобы модель корректнее работала с категориями.\n",
        "\n",
        "**Гипотеза 2: Создание агрегированных признаков на основе групп**\n",
        "\n",
        "Для каждой категории (loan_grade, loan_intent, person_home_ownership) вычислить статистики: средний процент дефолта, медианный доход, средняя сумма кредита в группе. Эти групповые признаки помогут модели уловить паттерны риска, характерные для определённых категорий клиентов, которые не видны по индивидуальным признакам.\n",
        "\n",
        "**Гипотеза 3: Критерий разбиения и Out-of-Bag валидация**\n",
        "\n",
        "Сравнить два критерия разбиения узлов: gini и entropy и использовать OOB (Out-of-Bag) оценку вместо стандартной валидации. OOB позволяет оценить качество модели на данных, не участвовавших в построении каждого дерева, без выделения отдельной валидационной выборки, что даст более честную оценку.\n",
        "\n",
        "**Гипотеза 4: Биннинг числовых признаков**\n",
        "\n",
        "Можно разбить непрерывные признаки (person_age, person_income, loan_amnt, loan_int_rate) на интервалы с помощью квантилей. Это может помочь модели лучше работать с нелинейными зависимостями и выявить пороговые значения, при которых резко меняется вероятность дефолта.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y1E-thlg2wNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Проверка гипотез**"
      ],
      "metadata": {
        "id": "EiM7NQst22El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гипотеза 1: препроцессинг (удаление выбросов + One-hot encoding)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Удаляем выбросы\n",
        "print(\"До удаления выбросов:\", len(df))\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "print(\"После удаления выбросов:\", len(df))\n",
        "\n",
        "# Заполняем пропуски только в числовых колонках\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# One-hot encoding для категориальных признаков\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем на признаки и целевой класс\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "print(f\"Количество признаков после One-hot: {X.shape[1]}\")\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем с n_estimators=100\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf.predict(X_test)\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Гипотеза 1: препроцессинг\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-u350zS_QMN",
        "outputId": "9302a0de-cb92-4e15-fe44-8144eca8598a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "До удаления выбросов: 32581\n",
            "После удаления выбросов: 31679\n",
            "Количество признаков после One-hot: 26\n",
            "Гипотеза 1: препроцессинг\n",
            "Accuracy: 0.9372\n",
            "Precision: 0.9699\n",
            "Recall: 0.7311\n",
            "F1-Score: 0.8338\n",
            "ROC-AUC: 0.9347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод по гипотезе 1:**\n",
        "\n",
        "Все метрики подросли, особенно Recall.\n",
        "\n",
        "Гипотезу 1 берём."
      ],
      "metadata": {
        "id": "gyVMNzpI_ar0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гипотеза 2: создание агрегированных признаков на основе групп\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Удаляем выбросы\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "\n",
        "# Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Создаём агрегированные признаки до one-hot encoding\n",
        "# Средний процент дефолта по категориям\n",
        "for col in ['loan_grade', 'loan_intent', 'person_home_ownership']:\n",
        "    default_rate = df.groupby(col)['loan_status'].mean()\n",
        "    df[f'{col}_default_rate'] = df[col].map(default_rate)\n",
        "\n",
        "# Медианный доход по категориям\n",
        "for col in ['loan_grade', 'person_home_ownership']:\n",
        "    median_income = df.groupby(col)['person_income'].median()\n",
        "    df[f'{col}_median_income'] = df[col].map(median_income)\n",
        "\n",
        "# Средняя сумма кредита по категориям\n",
        "for col in ['loan_intent', 'loan_grade']:\n",
        "    mean_loan = df.groupby(col)['loan_amnt'].mean()\n",
        "    df[f'{col}_mean_loan'] = df[col].map(mean_loan)\n",
        "\n",
        "print(\"Созданы агрегированные признаки:\")\n",
        "print(f\" - Средний процент дефолта по группам: 3 признака\")\n",
        "print(f\" - Медианный доход по группам: 2 признака\")\n",
        "print(f\" - Средняя сумма кредита по группам: 2 признака\")\n",
        "\n",
        "# One-hot encoding для категориальных признаков\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "print(f\"\\nКоличество признаков (с агрегированными): {X.shape[1]}\")\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf.predict(X_test)\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Гипотеза 2: агрегированные признаки\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCHXNxNr_fBU",
        "outputId": "af71387e-1a0c-4326-8ae7-85e5d9ca9fc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Созданы агрегированные признаки:\n",
            " - Средний процент дефолта по группам: 3 признака\n",
            " - Медианный доход по группам: 2 признака\n",
            " - Средняя сумма кредита по группам: 2 признака\n",
            "\n",
            "Количество признаков (с агрегированными): 33\n",
            "Гипотеза 2: агрегированные признаки\n",
            "Accuracy: 0.9377\n",
            "Precision: 0.9736\n",
            "Recall: 0.7304\n",
            "F1-Score: 0.8347\n",
            "ROC-AUC: 0.9306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выводы по гипотезе 2:**\n",
        "\n",
        "Результаты почти на том же уровне, что и в гипотезе 1. Precision чуть подрос, но Recall практически не изменился, смысла брать это в улучшенный бейзлайн не вижу.\n",
        "\n",
        "Гипотеза 2 не подтверждена."
      ],
      "metadata": {
        "id": "9rGRtYnl_sct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гипотеза 3: критерий разбиения и Out-of-Bag валидация\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Удаляем выбросы\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "\n",
        "# Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Сравниваем gini и entropy с OOB\n",
        "print(\"Проверяем разные критерии разбиения с OOB-оценкой:\\n\")\n",
        "\n",
        "for criterion in ['gini', 'entropy']:\n",
        "    rf = RandomForestClassifier(n_estimators=100, criterion=criterion, oob_score=True, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = rf.predict(X_test)\n",
        "    y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(f\"Criterion: {criterion}\")\n",
        "    print(f\"  OOB Score: {rf.oob_score_:.4f}\")\n",
        "    print(f\"  Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"  Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"  Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"  F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"  ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIcFETqU_wXt",
        "outputId": "d1598d01-aa0a-4813-fff9-02c4e621a9c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проверяем разные критерии разбиения с OOB-оценкой:\n",
            "\n",
            "Criterion: gini\n",
            "  OOB Score: 0.9330\n",
            "  Accuracy: 0.9372\n",
            "  Precision: 0.9699\n",
            "  Recall: 0.7311\n",
            "  F1-Score: 0.8338\n",
            "  ROC-AUC: 0.9347\n",
            "Criterion: entropy\n",
            "  OOB Score: 0.9330\n",
            "  Accuracy: 0.9373\n",
            "  Precision: 0.9736\n",
            "  Recall: 0.7289\n",
            "  F1-Score: 0.8337\n",
            "  ROC-AUC: 0.9370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод по гипотезе 3:**\n",
        "\n",
        "Критерий разбиения не даёт существенного улучшения. Можно оставить дефолтный gini или взять entropy, так как разница копеечная.\n",
        "\n",
        "Гипотеза 3 не подтвердилась, исключаем из улучшенного бейзлайна."
      ],
      "metadata": {
        "id": "RWxvt15NFDGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гипотеза 4: биннинг числовых признаков\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Удаляем выбросы\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "\n",
        "# Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Создаём бины для числовых признаков до one-hot encoding\n",
        "df['person_age_bin'] = pd.qcut(df['person_age'], q=5, labels=False, duplicates='drop')\n",
        "df['person_income_bin'] = pd.qcut(df['person_income'], q=5, labels=False, duplicates='drop')\n",
        "df['loan_amnt_bin'] = pd.qcut(df['loan_amnt'], q=5, labels=False, duplicates='drop')\n",
        "df['loan_int_rate_bin'] = pd.qcut(df['loan_int_rate'], q=5, labels=False, duplicates='drop')\n",
        "\n",
        "print(\"Созданы биннированные признаки:\")\n",
        "print(f\" - person_age_bin (5 интервалов)\")\n",
        "print(f\" - person_income_bin (5 интервалов)\")\n",
        "print(f\" - loan_amnt_bin (5 интервалов)\")\n",
        "print(f\" - loan_int_rate_bin (5 интервалов)\")\n",
        "\n",
        "# One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "print(f\"\\nКоличество признаков (с биннингом): {X.shape[1]}\")\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf.predict(X_test)\n",
        "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Гипотеза 4: биннинг числовых признаков\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgWt_w8iCcEG",
        "outputId": "52713fff-6f96-47c7-a240-353f8fe283df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Созданы биннированные признаки:\n",
            " - person_age_bin (5 интервалов)\n",
            " - person_income_bin (5 интервалов)\n",
            " - loan_amnt_bin (5 интервалов)\n",
            " - loan_int_rate_bin (5 интервалов)\n",
            "\n",
            "Количество признаков (с биннингом): 30\n",
            "Гипотеза 4: биннинг числовых признаков\n",
            "Accuracy: 0.9373\n",
            "Precision: 0.9672\n",
            "Recall: 0.7341\n",
            "F1-Score: 0.8347\n",
            "ROC-AUC: 0.9354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод по гипотезе 4:**\n",
        "\n",
        "Биннинг числовых признаков показал хороший результат. Recall немного вырос с 0.7311 до 0.7341, что означает модель чуть лучше ловит дефолты. ROC-AUC тоже улучшился с 0.9347 до 0.9354. Разбиение на интервалы помогло модели выявить пороговые значения, при которых меняется риск дефолта. Добавил 4 новых признака (бины для возраста, дохода, суммы кредита и процентной ставки).\n",
        "\n",
        "Гипотеза 4 подтверждена.\n"
      ],
      "metadata": {
        "id": "w-SaB-tyv7oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c., d.и e. Формирование улучшенного бейзлайна, обучение обучение и оценка качества модели**"
      ],
      "metadata": {
        "id": "hitY9ZW72-ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Улучшенный бейзлайн\n",
        "# Гипотеза 1 (Препроцессинг) + гипотеза 4 (Биннинг)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Улучшение 1: Удаляем выбросы\n",
        "df = df[(df['person_age'] < 100) & (df['person_emp_length'] < 100)]\n",
        "\n",
        "# Улучшение 2: Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Улучшение 3: Биннинг числовых признаков\n",
        "df['person_age_bin'] = pd.qcut(df['person_age'], q=5, labels=False, duplicates='drop')\n",
        "df['person_income_bin'] = pd.qcut(df['person_income'], q=5, labels=False, duplicates='drop')\n",
        "df['loan_amnt_bin'] = pd.qcut(df['loan_amnt'], q=5, labels=False, duplicates='drop')\n",
        "df['loan_int_rate_bin'] = pd.qcut(df['loan_int_rate'], q=5, labels=False, duplicates='drop')\n",
        "\n",
        "# Улучшение 4: One-hot encoding\n",
        "df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'], drop_first=False)\n",
        "\n",
        "# Разделяем на признаки и целевой класс\n",
        "X = df.drop('loan_status', axis=1)\n",
        "y = df['loan_status']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Обучаем с улучшениями\n",
        "rf_improved = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_improved.fit(X_train, y_train)\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf_improved.predict(X_test)\n",
        "y_pred_proba = rf_improved.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Улучшенный бейзлайн\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzk2emXEDBYv",
        "outputId": "e5489f48-6b87-4b5e-ba7d-932c0d5e8084"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Улучшенный бейзлайн\n",
            "Accuracy: 0.9373\n",
            "Precision: 0.9672\n",
            "Recall: 0.7341\n",
            "F1-Score: 0.8347\n",
            "ROC-AUC: 0.9354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**f. Сравнение результатов с пунктом 2**\n",
        "\n",
        "**Базовый бейзлайн:**\n",
        "\n",
        "Accuracy: 0.9319\n",
        "\n",
        "Precision: 0.9693\n",
        "\n",
        "Recall: 0.7103\n",
        "\n",
        "F1-Score: 0.8198\n",
        "\n",
        "ROC-AUC: 0.9289\n",
        "\n",
        "**С улучшенным бейзлайном:**\n",
        "\n",
        "Accuracy: 0.9373\n",
        "\n",
        "Precision: 0.9672\n",
        "\n",
        "Recall: 0.7341\n",
        "\n",
        "F1-Score: 0.8347\n",
        "\n",
        "ROC-AUC: 0.9354\n",
        "\n",
        "Улучшения есть по всем метрикам. Больше всего вырос Recall, модель теперь лучше находит тех, кто может не вернуть кредит. Остальные метрики тоже немного подросли."
      ],
      "metadata": {
        "id": "pmF7pxXn3ot1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**g. Выводы**\n",
        "\n",
        "Взял два улучшения, убрал странные значения в данных, использовал One-hot encoding, разбил числовые признаки на группы, ну и в результате все метрики выросли, особенно Recall (с 0.71 до 0.73)."
      ],
      "metadata": {
        "id": "IOupt5Ad3w7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Имплементация алгоритма машинного обучения"
      ],
      "metadata": {
        "id": "Fdxuv_UQ1Tru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Имплементация случайного леса**"
      ],
      "metadata": {
        "id": "suHUXcym4CNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTreeClassifierCustom:\n",
        "\n",
        "    class _Node:\n",
        "        __slots__ = (\"is_leaf\", \"pred_class\", \"proba\", \"feature\", \"threshold\", \"left\", \"right\")\n",
        "        def __init__(self, is_leaf, pred_class=None, proba=None, feature=None, threshold=None, left=None, right=None):\n",
        "            self.is_leaf = is_leaf\n",
        "            self.pred_class = pred_class\n",
        "            self.proba = proba          # вектор вероятностей по self.classes_\n",
        "            self.feature = feature\n",
        "            self.threshold = threshold\n",
        "            self.left = left\n",
        "            self.right = right\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion=\"gini\",\n",
        "                 max_features=\"sqrt\", random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = int(min_samples_split)\n",
        "        self.criterion = criterion\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.rng_ = np.random.RandomState(random_state)\n",
        "\n",
        "        self.root_ = None\n",
        "        self.classes_ = None\n",
        "        self.class_to_index_ = None\n",
        "\n",
        "    def _impurity(self, y_idx, sample_weight=None):\n",
        "        \"\"\"Импурити узла по y в индексах классов.\"\"\"\n",
        "        if y_idx.size == 0:\n",
        "            return 0.0\n",
        "\n",
        "        if sample_weight is None:\n",
        "            counts = np.bincount(y_idx, minlength=len(self.classes_)).astype(float)\n",
        "        else:\n",
        "            counts = np.zeros(len(self.classes_), dtype=float)\n",
        "            for c in range(len(self.classes_)):\n",
        "                mask = (y_idx == c)\n",
        "                counts[c] = sample_weight[mask].sum()\n",
        "\n",
        "        total = counts.sum()\n",
        "        if total <= 0:\n",
        "            return 0.0\n",
        "        p = counts / total\n",
        "\n",
        "        if self.criterion == \"gini\":\n",
        "            return 1.0 - np.sum(p * p)\n",
        "        elif self.criterion == \"entropy\":\n",
        "            eps = 1e-12\n",
        "            return -np.sum(p * np.log2(p + eps))\n",
        "        else:\n",
        "            raise ValueError(\"criterion must be 'gini' or 'entropy'\")\n",
        "\n",
        "    def _leaf_stats(self, y_idx, sample_weight=None):\n",
        "        \"\"\"Возвращает (pred_class, proba_vector).\"\"\"\n",
        "        if y_idx.size == 0:\n",
        "            proba = np.ones(len(self.classes_), dtype=float) / len(self.classes_)\n",
        "            pred = self.classes_[0]\n",
        "            return pred, proba\n",
        "\n",
        "        if sample_weight is None:\n",
        "            counts = np.bincount(y_idx, minlength=len(self.classes_)).astype(float)\n",
        "        else:\n",
        "            counts = np.zeros(len(self.classes_), dtype=float)\n",
        "            for c in range(len(self.classes_)):\n",
        "                mask = (y_idx == c)\n",
        "                counts[c] = sample_weight[mask].sum()\n",
        "\n",
        "        total = counts.sum()\n",
        "        if total <= 0:\n",
        "            proba = np.ones(len(self.classes_), dtype=float) / len(self.classes_)\n",
        "        else:\n",
        "            proba = counts / total\n",
        "\n",
        "        pred_class = self.classes_[int(np.argmax(counts))]\n",
        "        return pred_class, proba\n",
        "\n",
        "    def _n_features_to_take(self, n_features):\n",
        "        if self.max_features in (None, \"all\"):\n",
        "            return n_features\n",
        "        if self.max_features == \"sqrt\":\n",
        "            return max(1, int(np.sqrt(n_features)))\n",
        "        if self.max_features == \"log2\":\n",
        "            return max(1, int(np.log2(n_features)))\n",
        "        # Если число\n",
        "        k = int(self.max_features)\n",
        "        return max(1, min(n_features, k))\n",
        "\n",
        "    def _best_split(self, X, y_idx, sample_weight=None):\n",
        "        \"\"\"Ищем лучший сплит: (best_feature, best_threshold, best_gain).\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return None, None, 0.0\n",
        "\n",
        "        parent_imp = self._impurity(y_idx, sample_weight)\n",
        "        if parent_imp <= 1e-12:\n",
        "            return None, None, 0.0\n",
        "\n",
        "        k = self._n_features_to_take(n_features)\n",
        "        feat_candidates = self.rng_.choice(n_features, size=k, replace=False)\n",
        "\n",
        "        best_gain = 0.0\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Для ускорения: считаем веса узла один раз\n",
        "        if sample_weight is None:\n",
        "            total_w = float(n_samples)\n",
        "        else:\n",
        "            total_w = float(sample_weight.sum())\n",
        "            if total_w <= 0:\n",
        "                return None, None, 0.0\n",
        "\n",
        "        for f in feat_candidates:\n",
        "            x = X[:, f]\n",
        "            uniq = np.unique(x)\n",
        "            if uniq.size <= 1:\n",
        "                continue\n",
        "\n",
        "            # Пороги середины между соседними уникальными значениями\n",
        "            thresholds = (uniq[:-1] + uniq[1:]) / 2.0\n",
        "\n",
        "            for thr in thresholds:\n",
        "                left_mask = x <= thr\n",
        "                right_mask = ~left_mask\n",
        "                if not left_mask.any() or not right_mask.any():\n",
        "                    continue\n",
        "\n",
        "                y_left = y_idx[left_mask]\n",
        "                y_right = y_idx[right_mask]\n",
        "\n",
        "                if sample_weight is None:\n",
        "                    w_left = float(y_left.size)\n",
        "                    w_right = float(y_right.size)\n",
        "                    sw_left = None\n",
        "                    sw_right = None\n",
        "                else:\n",
        "                    sw_left = sample_weight[left_mask]\n",
        "                    sw_right = sample_weight[right_mask]\n",
        "                    w_left = float(sw_left.sum())\n",
        "                    w_right = float(sw_right.sum())\n",
        "                    if w_left <= 0 or w_right <= 0:\n",
        "                        continue\n",
        "\n",
        "                imp_left = self._impurity(y_left, sw_left)\n",
        "                imp_right = self._impurity(y_right, sw_right)\n",
        "\n",
        "                child_imp = (w_left / total_w) * imp_left + (w_right / total_w) * imp_right\n",
        "                gain = parent_imp - child_imp\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = f\n",
        "                    best_threshold = thr\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def _build(self, X, y_idx, depth, sample_weight=None):\n",
        "        # Условия остановки\n",
        "        if (self.max_depth is not None) and (depth >= self.max_depth):\n",
        "            pred, proba = self._leaf_stats(y_idx, sample_weight)\n",
        "            return self._Node(is_leaf=True, pred_class=pred, proba=proba)\n",
        "\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            pred, proba = self._leaf_stats(y_idx, sample_weight)\n",
        "            return self._Node(is_leaf=True, pred_class=pred, proba=proba)\n",
        "\n",
        "        # Если все одного класса - лист\n",
        "        if np.unique(y_idx).size == 1:\n",
        "            pred, proba = self._leaf_stats(y_idx, sample_weight)\n",
        "            return self._Node(is_leaf=True, pred_class=pred, proba=proba)\n",
        "\n",
        "        f, thr, gain = self._best_split(X, y_idx, sample_weight)\n",
        "        if f is None or gain <= 0:\n",
        "            pred, proba = self._leaf_stats(y_idx, sample_weight)\n",
        "            return self._Node(is_leaf=True, pred_class=pred, proba=proba)\n",
        "\n",
        "        left_mask = X[:, f] <= thr\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        X_left, X_right = X[left_mask], X[right_mask]\n",
        "        y_left, y_right = y_idx[left_mask], y_idx[right_mask]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            sw_left = None\n",
        "            sw_right = None\n",
        "        else:\n",
        "            sw_left = sample_weight[left_mask]\n",
        "            sw_right = sample_weight[right_mask]\n",
        "\n",
        "        left = self._build(X_left, y_left, depth + 1, sw_left)\n",
        "        right = self._build(X_right, y_right, depth + 1, sw_right)\n",
        "\n",
        "        return self._Node(is_leaf=False, feature=f, threshold=thr, left=left, right=right)\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.class_to_index_ = {c: i for i, c in enumerate(self.classes_)}\n",
        "        y_idx = np.array([self.class_to_index_[v] for v in y], dtype=int)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = np.asarray(sample_weight, dtype=float)\n",
        "            if sample_weight.shape[0] != X.shape[0]:\n",
        "                raise ValueError(\"sample_weight must have shape (n_samples,)\")\n",
        "\n",
        "        self.root_ = self._build(X, y_idx, depth=0, sample_weight=sample_weight)\n",
        "        return self\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        while not node.is_leaf:\n",
        "            if x[node.feature] <= node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node.pred_class, node.proba\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        preds = []\n",
        "        for i in range(X.shape[0]):\n",
        "            pred, _ = self._predict_one(X[i], self.root_)\n",
        "            preds.append(pred)\n",
        "        return np.asarray(preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        probas = np.zeros((X.shape[0], len(self.classes_)), dtype=float)\n",
        "        for i in range(X.shape[0]):\n",
        "            _, p = self._predict_one(X[i], self.root_)\n",
        "            probas[i] = p\n",
        "        return probas\n",
        "\n",
        "\n",
        "class RandomForestClassifierCustom:\n",
        "\n",
        "\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n",
        "                 max_features=\"sqrt\", criterion=\"gini\", random_state=None,\n",
        "                 balanced=False):\n",
        "        self.n_estimators = int(n_estimators)\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = int(min_samples_split)\n",
        "        self.max_features = max_features\n",
        "        self.criterion = criterion\n",
        "        self.random_state = random_state\n",
        "        self.balanced = bool(balanced)\n",
        "\n",
        "        self.rng_ = np.random.RandomState(random_state)\n",
        "        self.trees_ = []\n",
        "        self.classes_ = None\n",
        "\n",
        "    def _bootstrap_indices(self, y):\n",
        "        n = y.shape[0]\n",
        "        if not self.balanced:\n",
        "            return self.rng_.randint(0, n, size=n)\n",
        "\n",
        "        # Balanced bootstrap: набираем одинаковое число объектов каждого класса\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        n_classes = classes.size\n",
        "        per_class = int(np.ceil(n / n_classes))\n",
        "\n",
        "        idx_all = []\n",
        "        for c in classes:\n",
        "            idx_c = np.where(y == c)[0]\n",
        "            # с возвращением (oversampling minority)\n",
        "            idx_sampled = self.rng_.choice(idx_c, size=per_class, replace=True)\n",
        "            idx_all.append(idx_sampled)\n",
        "\n",
        "        idx = np.concatenate(idx_all)\n",
        "        # Приводим размер к n\n",
        "        if idx.size > n:\n",
        "            idx = self.rng_.choice(idx, size=n, replace=False)\n",
        "        elif idx.size < n:\n",
        "            extra = self.rng_.choice(idx, size=(n - idx.size), replace=True)\n",
        "            idx = np.concatenate([idx, extra])\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y)\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        self.trees_ = []\n",
        "        for i in range(self.n_estimators):\n",
        "            idx = self._bootstrap_indices(y)\n",
        "            Xb = X[idx]\n",
        "            yb = y[idx]\n",
        "\n",
        "            # Разные random_state для деревьев\n",
        "            tree = DecisionTreeClassifierCustom(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                criterion=self.criterion,\n",
        "                max_features=self.max_features,\n",
        "                random_state=int(self.rng_.randint(0, 1_000_000_000))\n",
        "            )\n",
        "            tree.fit(Xb, yb)\n",
        "            self.trees_.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        # Согласуем порядок классов\n",
        "        proba_sum = np.zeros((X.shape[0], self.classes_.size), dtype=float)\n",
        "\n",
        "        for tree in self.trees_:\n",
        "            # tree.classes_ может совпадать, но на всякий случай делаем маппинг\n",
        "            p = tree.predict_proba(X)\n",
        "            # Приводим p к self.classes_ (на случай, если в бутстрэпе не было какого-то класса)\n",
        "            p_aligned = np.zeros_like(proba_sum)\n",
        "            for j, c in enumerate(tree.classes_):\n",
        "                jj = np.where(self.classes_ == c)[0][0]\n",
        "                p_aligned[:, jj] = p[:, j]\n",
        "            proba_sum += p_aligned\n",
        "\n",
        "        proba = proba_sum / max(1, len(self.trees_))\n",
        "        # Нормировка\n",
        "        row_sums = proba.sum(axis=1, keepdims=True)\n",
        "        row_sums[row_sums == 0] = 1.0\n",
        "        return proba / row_sums\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return self.classes_[np.argmax(proba, axis=1)]\n"
      ],
      "metadata": {
        "id": "E_LeUDTEDgiJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Обучение и оценка качества имплементированной модели с базовым бейзлайном**"
      ],
      "metadata": {
        "id": "UkFlLkFT4WrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Загружаем датасет\n",
        "df_base = pd.read_csv('credit_risk_dataset.csv')\n",
        "\n",
        "# Базовый препроцессинг: Label Encoding для категориальных признаков\n",
        "for col in ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']:\n",
        "    le = LabelEncoder()\n",
        "    df_base[col] = le.fit_transform(df_base[col])\n",
        "\n",
        "# Заполняем пропуски медианой\n",
        "df_base = df_base.fillna(df_base.median())\n",
        "\n",
        "# Разделяем на признаки и целевой класс\n",
        "X_base = df_base.drop('loan_status', axis=1)\n",
        "y_base = df_base['loan_status']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(\n",
        "    X_base, y_base, test_size=0.2, random_state=42, stratify=y_base\n",
        ")\n",
        "\n",
        "# Нормализуем признаки\n",
        "scaler_base = StandardScaler()\n",
        "X_train_base = scaler_base.fit_transform(X_train_base)\n",
        "X_test_base = scaler_base.transform(X_test_base)\n",
        "\n",
        "# Конвертируем в numpy\n",
        "X_train_base = np.array(X_train_base)\n",
        "X_test_base = np.array(X_test_base)\n",
        "y_train_base = np.array(y_train_base)\n",
        "y_test_base = np.array(y_test_base)\n",
        "\n",
        "print(f\"Форма train: {X_train_base.shape}, test: {X_test_base.shape}\")\n",
        "\n",
        "# Обучаем на базовом бейзлайне\n",
        "rf_base_custom = RandomForestClassifierCustom(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    balanced=False\n",
        ")\n",
        "\n",
        "rf_base_custom.fit(X_train_base, y_train_base)\n",
        "\n",
        "print(\"Имплементированный случайный лес (базовый бейзлайн) обучен\")\n",
        "\n",
        "# Предсказания\n",
        "y_pred_base_custom = rf_base_custom.predict(X_test_base)\n",
        "y_pred_proba_base_custom = rf_base_custom.predict_proba(X_test_base)[:, 1]\n",
        "\n",
        "# Метрики\n",
        "print(\"Имплементированный случвайный лес (базовый бейзлайн)\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_base, y_pred_base_custom):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_base, y_pred_base_custom):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test_base, y_pred_base_custom):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test_base, y_pred_base_custom):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test_base, y_pred_proba_base_custom):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qWyDjF7FqsG",
        "outputId": "2d40b702-6b04-4377-c5d2-e06c88519b3e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Форма train: (26064, 11), test: (6517, 11)\n",
            "Имплементированный случайный лес (базовый бейзлайн) обучен\n",
            "Имплементированный случвайный лес (базовый бейзлайн)\n",
            "Accuracy: 0.9309\n",
            "Precision: 0.9629\n",
            "Recall: 0.7110\n",
            "F1-Score: 0.8180\n",
            "ROC-AUC: 0.9280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Сравнение с пунктом 2**\n",
        "\n",
        "Sklearn:​\n",
        "\n",
        "Accuracy: 0.9319​\n",
        "\n",
        "Precision: 0.9693​\n",
        "\n",
        "Recall: 0.7103​\n",
        "\n",
        "F1-Score: 0.8198​\n",
        "\n",
        "ROC-AUC: 0.9289​\n",
        "\n",
        "Имплементированный случайный лес:\n",
        "\n",
        "Accuracy: 0.9309\n",
        "\n",
        "Precision: 0.9629\n",
        "\n",
        "Recall: 0.7110\n",
        "\n",
        "F1-Score: 0.8180\n",
        "\n",
        "ROC-AUC: 0.9280\n",
        "\n",
        "Имплементированная модель практически повторила качество sklearn на базовом бейзлайне: Accuracy, Recall, F1 и ROC-AUC отличаются минимально."
      ],
      "metadata": {
        "id": "2T-R-KbR4v5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e. Выводы**\n",
        "\n",
        "В целом, мой вариант алгоритма на базовом бейзлайне получился почти на уровне sklearn: основные метрики очень близкие, так что реализация работает нормально."
      ],
      "metadata": {
        "id": "9stXO4M_43dI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**f., g., и h. Добавление техник из улучшенного бейзлайна, обучение и оценка качества имплементированной модели**"
      ],
      "metadata": {
        "id": "T2ziDbZp4-WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Имплементированный случайный с улучшенным бейзлайном\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "# Улучшенный препроцессинг (удаление выбросов + биннинг + one-hot)\n",
        "df = pd.read_csv(\"credit_risk_dataset.csv\")\n",
        "\n",
        "# Удаляем выбросы\n",
        "df = df[(df[\"person_age\"] < 100) & (df[\"person_emp_length\"] < 100)]\n",
        "\n",
        "# Заполняем пропуски (только числовые колонки)\n",
        "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Биннинг числовых признаков\n",
        "df[\"person_age_bin\"] = pd.qcut(df[\"person_age\"], q=5, labels=False, duplicates=\"drop\")\n",
        "df[\"person_income_bin\"] = pd.qcut(df[\"person_income\"], q=5, labels=False, duplicates=\"drop\")\n",
        "df[\"loan_amnt_bin\"] = pd.qcut(df[\"loan_amnt\"], q=5, labels=False, duplicates=\"drop\")\n",
        "df[\"loan_int_rate_bin\"] = pd.qcut(df[\"loan_int_rate\"], q=5, labels=False, duplicates=\"drop\")\n",
        "\n",
        "# One-hot encoding категориальных\n",
        "cat_cols = [\"person_home_ownership\", \"loan_intent\", \"loan_grade\", \"cb_person_default_on_file\"]\n",
        "df = pd.get_dummies(df, columns=cat_cols, drop_first=False)\n",
        "\n",
        "# Признаки и таргет\n",
        "X = df.drop(\"loan_status\", axis=1)\n",
        "y = df[\"loan_status\"]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Нормализация\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Конвертация в numpy\n",
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)\n",
        "y_train = np.asarray(y_train)\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "print(f\"Форма train: {X_train.shape}, test: {X_test.shape}\")\n",
        "\n",
        "# Обучаем с улучшениями\n",
        "rf_improved_custom = RandomForestClassifierCustom(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_improved_custom.fit(X_train, y_train)\n",
        "print(\"Кастомный Random Forest с улучшениями обучен\")\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf_improved_custom.predict(X_test)\n",
        "\n",
        "print(f\"Размер тестового набора: {len(X_test)}\")\n",
        "print(f\"Первые 10 предсказаний: {y_pred[:10]}\")\n",
        "\n",
        "# Вероятности для ROC-AUC\n",
        "proba = rf_improved_custom.predict_proba(X_test)\n",
        "\n",
        "# На всякий случай делаем выбор вероятности именно класса 1 (если классы не [0, 1] по порядку)\n",
        "if hasattr(rf_improved_custom, \"classes_\"):\n",
        "    pos_idx = int(np.where(rf_improved_custom.classes_ == 1)[0][0])\n",
        "else:\n",
        "    pos_idx = 1  # стандартный случай: столбец 1 — класс \"1\"\n",
        "\n",
        "y_pred_proba = proba[:, pos_idx]\n",
        "\n",
        "print(\"Имплементированный Random Forest с улучшениями\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t1iB1tnH7YL",
        "outputId": "c5cf9108-223b-49e8-cb82-233127732889"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Форма train: (25343, 30), test: (6336, 30)\n",
            "Кастомный Random Forest с улучшениями обучен\n",
            "Размер тестового набора: 6336\n",
            "Первые 10 предсказаний: [0 0 0 0 0 0 0 1 1 0]\n",
            "Имплементированный Random Forest с улучшениями\n",
            "Accuracy: 0.9361\n",
            "Precision: 0.9678\n",
            "Recall: 0.7275\n",
            "F1-Score: 0.8306\n",
            "ROC-AUC: 0.9367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**i. Сравнение результатов с пунктом 3**\n",
        "\n",
        "Sklearn: Accuracy 0.9373, Precision 0.9672, Recall 0.7341, F1-Score 0.8347, ROC-AUC 0.9354.\n",
        "\n",
        "Имплементированный: Accuracy 0.9361, Precision 0.9678, Recall 0.7275, F1-Score 0.8306, ROC-AUC 0.9367."
      ],
      "metadata": {
        "id": "TcfuPZEG5UOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**j. Выводы**\n",
        "\n",
        "После улучшенного препроцессинга моя реализация стала работать почти как sklearn: все метрики очень близкие, местами даже чуть лучше."
      ],
      "metadata": {
        "id": "I4SbGWNh5Yyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Регрессия"
      ],
      "metadata": {
        "id": "p7-rTKkQzcJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Создание бейзлайна и оценка качества"
      ],
      "metadata": {
        "id": "yNlSTxXU1qrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Обучение модели sklearn**"
      ],
      "metadata": {
        "id": "xEcaBanA2q33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['Company_Name', 'Job_Role', 'Experience_Level', 'City']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "\n",
        "# Заполняем пропуски медианой\n",
        "df = df.fillna(df.median())\n",
        "\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Обучение Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "# Вычисляем метрики\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "\n",
        "# Вывод метрик\n",
        "print(\"Случайный лес Бейзлайн\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5iFl057KADW",
        "outputId": "acb21fd5-c4aa-497f-f60d-58698b86ea47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Случайный лес Бейзлайн\n",
            "MAE: 417367.38\n",
            "RMSE: 536094.91\n",
            "R²: 0.5435\n",
            "MAPE: 40.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Оценка качества модели**\n",
        "\n",
        "Случайный лес показывает неплохие результаты на базовом бейзлайне. R² равен 0.5435, то есть модель объясняет 54% разброса зарплат, что намного лучше чем у других моделей. MAPE составляет 40%, значит модель в среднем ошибается на 40% от реальной зарплаты - это довольно много, но для бейзлайна приемлемо. MAE около 417 тысяч рупий показывает среднюю абсолютную ошибку, а RMSE 536 тысяч указывает на то, что есть случаи с большими промахами."
      ],
      "metadata": {
        "id": "HGN4Req-2q33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Улучшение бейзлайна"
      ],
      "metadata": {
        "id": "kV5ObDig1spe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Формулирование гипотез**\n",
        "\n",
        "**Гипотеза 1: Препроцессинг**\n",
        "\n",
        "Удалить выбросы в зарплатах (оставить данные между 1-м и 99-м перцентилем, чтобы убрать экстремальные значения)\n",
        "\n",
        "Использовать One-hot encoding вместо Label Encoding для категориальных признаков (Company_Name, Job_Role, City, Experience_Level), потому что Random Forest лучше работает с бинарными признаками\n",
        "\n",
        "**Гипотеза 2: Подбор гиперпараметров**\n",
        "\n",
        "Найти оптимальное количество деревьев через кросс-валидацию - проверить значения от 50 до 300\n",
        "\n",
        "Подобрать максимальную глубину деревьев - проверить 10, 20, 30 и None\n",
        "\n",
        "Подобрать min_samples_split для контроля переобучения и проверить 2, 5, 10, 20\n",
        "\n",
        "**Гипотеза 3: Новые признаки**\n",
        "\n",
        "Создать агрегированные признаки: среднюю зарплату по городу (City_Mean_Salary), среднюю зарплату по должности (Job_Mean_Salary), среднюю зарплату по уровню опыта (Exp_Mean_Salary)\n",
        "\n",
        "Добавить взаимодействие между Demand_Index и Remote_Option_Flag (их произведение), чтобы учесть связь спроса и удалённой работы\n",
        "\n",
        "**Гипотеза 4: Убрать масштабирование**\n",
        "\n",
        "Случайный лес не требует нормализации данных, так как работает с деревьями решений (разбиение по порогам), а не с расстояниями. Попробовать обучить без StandardScaler и сравнить результаты"
      ],
      "metadata": {
        "id": "oBrfxm3C37xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Проверка гипотез**"
      ],
      "metadata": {
        "id": "NFeFrv8Y37xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гипотеза 1: препроцессинг (удаление выбросов + One-hot encoding)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "\n",
        "# Удаляем выбросы\n",
        "print(\"До удаления выбросов:\", len(df))\n",
        "q1 = df['Salary_INR'].quantile(0.01)\n",
        "q99 = df['Salary_INR'].quantile(0.99)\n",
        "df = df[(df['Salary_INR'] >= q1) & (df['Salary_INR'] <= q99)]\n",
        "print(\"После удаления выбросов:\", len(df))\n",
        "\n",
        "\n",
        "# Заполняем пропуски только в числовых колонках\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "\n",
        "# One-hot encoding для категориальных признаков\n",
        "df = pd.get_dummies(df, columns=['Company_Name', 'Job_Role', 'Experience_Level', 'City'], drop_first=False)\n",
        "\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "\n",
        "print(f\"Количество признаков после One-hot: {X.shape[1]}\")\n",
        "\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Обучаем Random Forest с n_estimators=100\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "\n",
        "# Метрики\n",
        "print(\"Гипотеза 1: препроцессинг\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK4a60geMfE6",
        "outputId": "f9bc3bcf-f178-4de6-9a3c-d0df192d3e52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "До удаления выбросов: 30000\n",
            "После удаления выбросов: 29400\n",
            "Количество признаков после One-hot: 66\n",
            "Гипотеза 1: препроцессинг\n",
            "MAE: 407834.62\n",
            "RMSE: 519416.93\n",
            "R²: 0.4861\n",
            "MAPE: 40.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод по Гипотезе 1:**\n",
        "\n",
        "Результаты неоднозначные, MAE и RMSE немного улучшились, но R² упал с 0.5435 до 0.4861 - модель хуже объясняет данные. MAPE практически не изменился. Удаление выбросов убрало 600 записей. One-hot encoding увеличил количество признаков до 66\n",
        "\n",
        "Гипотеза 1 не подтверждена, результаты хуже базового бейзлайна по R². Не используем."
      ],
      "metadata": {
        "id": "_9pmg83ENEHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гипотеза 2: подбор гиперпараметров\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['Company_Name', 'Job_Role', 'Experience_Level', 'City']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "\n",
        "# Заполняем пропуски медианой\n",
        "df = df.fillna(df.median())\n",
        "\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Проверяем разные значения n_estimators\n",
        "n_estimators_values = [50, 100, 150, 200, 300]\n",
        "results = []\n",
        "\n",
        "print(\"Проверка разных значений n_estimators с кросс-валидацией (3-fold):\")\n",
        "\n",
        "for n in n_estimators_values:\n",
        "    rf = RandomForestRegressor(n_estimators=n, random_state=42, n_jobs=-1)\n",
        "    cv_scores = cross_val_score(rf, X_train, y_train, cv=3, scoring='r2')\n",
        "    mean_score = cv_scores.mean()\n",
        "    results.append({'n_estimators': n, 'mean_r2': mean_score})\n",
        "    print(f\"n_estimators={n:3d}: R² (3-fold CV) = {mean_score:.4f}\")\n",
        "\n",
        "# Находим лучшее n_estimators\n",
        "best_n = max(results, key=lambda x: x['mean_r2'])['n_estimators']\n",
        "best_r2_cv = max(results, key=lambda x: x['mean_r2'])['mean_r2']\n",
        "\n",
        "print(f\"\\nЛучшее n_estimators = {best_n} (R² = {best_r2_cv:.4f})\")\n",
        "\n",
        "\n",
        "# Обучаем Random Forest с лучшим n_estimators\n",
        "rf_best = RandomForestRegressor(n_estimators=best_n, random_state=42)\n",
        "rf_best.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_best.predict(X_test)\n",
        "\n",
        "print(f\"\\nГипотеза 2: подбор гиперпараметров (n_estimators={best_n})\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8UTKKd8Nb6q",
        "outputId": "4835e9cc-78a1-4d10-d77d-f0a91e195d8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проверка разных значений n_estimators с кросс-валидацией (3-fold):\n",
            "n_estimators= 50: R² (3-fold CV) = 0.5181\n",
            "n_estimators=100: R² (3-fold CV) = 0.5213\n",
            "n_estimators=150: R² (3-fold CV) = 0.5229\n",
            "n_estimators=200: R² (3-fold CV) = 0.5237\n",
            "n_estimators=300: R² (3-fold CV) = 0.5246\n",
            "\n",
            "Лучшее n_estimators = 300 (R² = 0.5246)\n",
            "\n",
            "Гипотеза 2: подбор гиперпараметров (n_estimators=300)\n",
            "MAE: 417065.27\n",
            "RMSE: 535401.37\n",
            "R²: 0.5447\n",
            "MAPE: 40.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод по Гипотезе 2:**\n",
        "\n",
        "n_estimators=300 лучше, чем n_estimators=100, R² чуть вырос с 0.5435 до 0.5447, MAE и RMSE немного улучшились, MAPE остался на том же уровне, Кросс-валидация подтвердила, что больше деревьев = лучше качество, улучшение несильное, но берём.\n",
        "\n",
        "Гипотеза 2 подтверждена, n_estimators=300 добавляем в улучшенный бейзлайн."
      ],
      "metadata": {
        "id": "aKKQLUz8OkN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гипотеза 3: новые признаки\n",
        "\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "\n",
        "# Заполняем пропуски только в числовых колонках\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "\n",
        "# Создаём новые признаки до кодирования\n",
        "# Средняя зарплата по городу\n",
        "city_mean_salary = df.groupby('City')['Salary_INR'].mean()\n",
        "df['City_Mean_Salary'] = df['City'].map(city_mean_salary)\n",
        "\n",
        "# Средняя зарплата по должности\n",
        "job_mean_salary = df.groupby('Job_Role')['Salary_INR'].mean()\n",
        "df['Job_Mean_Salary'] = df['Job_Role'].map(job_mean_salary)\n",
        "\n",
        "# Средняя зарплата по уровню опыта\n",
        "exp_mean_salary = df.groupby('Experience_Level')['Salary_INR'].mean()\n",
        "df['Exp_Mean_Salary'] = df['Experience_Level'].map(exp_mean_salary)\n",
        "\n",
        "# Взаимодействие Demand_Index и Remote_Option_Flag\n",
        "df['Demand_Remote_Interaction'] = df['Demand_Index'] * df['Remote_Option_Flag']\n",
        "\n",
        "print(f\"Новые признаки созданы:\")\n",
        "print(f\" - City_Mean_Salary: min={df['City_Mean_Salary'].min():.2f}, max={df['City_Mean_Salary'].max():.2f}\")\n",
        "print(f\" - Job_Mean_Salary: min={df['Job_Mean_Salary'].min():.2f}, max={df['Job_Mean_Salary'].max():.2f}\")\n",
        "print(f\" - Exp_Mean_Salary: min={df['Exp_Mean_Salary'].min():.2f}, max={df['Exp_Mean_Salary'].max():.2f}\")\n",
        "print(f\" - Demand_Remote_Interaction: min={df['Demand_Remote_Interaction'].min():.2f}, max={df['Demand_Remote_Interaction'].max():.2f}\")\n",
        "\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['Company_Name', 'Job_Role', 'Experience_Level', 'City']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "\n",
        "print(f\"\\nКоличество признаков (без новых): 6\")\n",
        "print(f\"Количество признаков (с новыми): {X.shape[1]}\")\n",
        "\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Обучаем случаный лес с n_estimators=300\n",
        "rf_fe = RandomForestRegressor(n_estimators=300, random_state=42)\n",
        "rf_fe.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_fe.predict(X_test)\n",
        "\n",
        "print(\"\\nГипотеза 3: новые признаки (n_estimators=300)\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYsWDkyoOxgc",
        "outputId": "7ab4575b-daf0-4900-c84d-b10abf2c70f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Новые признаки созданы:\n",
            " - City_Mean_Salary: min=1271842.23, max=1315864.30\n",
            " - Job_Mean_Salary: min=747078.48, max=2840625.75\n",
            " - Exp_Mean_Salary: min=1276746.62, max=1302494.71\n",
            " - Demand_Remote_Interaction: min=0.00, max=99.00\n",
            "\n",
            "Количество признаков (без новых): 6\n",
            "Количество признаков (с новыми): 10\n",
            "\n",
            "Гипотеза 3: новые признаки (n_estimators=300)\n",
            "MAE: 414923.29\n",
            "RMSE: 532488.37\n",
            "R²: 0.5496\n",
            "MAPE: 40.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод по Гипотезе 3:**\n",
        "\n",
        "Новые признаки улучшили качество модели, R² вырос с 0.5447 до 0.5496, MAE и RMSE снизились, MAPE немного улучшился, средние зарплаты по группам дали модели полезную информацию, взаимодействие Demand_Index и Remote_Option_Flag тоже помогло\n",
        "\n",
        "Гипотеза 3 подтверждена, добавляем новые признаки в улучшенный бейзлайн."
      ],
      "metadata": {
        "id": "56CanNfQPNSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гипотеза 4: подбор max_depth\n",
        "\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "\n",
        "# Заполняем пропуски только в числовых колонках\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "\n",
        "# Создаём новые признаки\n",
        "city_mean_salary = df.groupby('City')['Salary_INR'].mean()\n",
        "df['City_Mean_Salary'] = df['City'].map(city_mean_salary)\n",
        "\n",
        "job_mean_salary = df.groupby('Job_Role')['Salary_INR'].mean()\n",
        "df['Job_Mean_Salary'] = df['Job_Role'].map(job_mean_salary)\n",
        "\n",
        "exp_mean_salary = df.groupby('Experience_Level')['Salary_INR'].mean()\n",
        "df['Exp_Mean_Salary'] = df['Experience_Level'].map(exp_mean_salary)\n",
        "\n",
        "df['Demand_Remote_Interaction'] = df['Demand_Index'] * df['Remote_Option_Flag']\n",
        "\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['Company_Name', 'Job_Role', 'Experience_Level', 'City']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Проверяем разные значения max_depth\n",
        "max_depth_values = [10, 20, 30, None]\n",
        "results = []\n",
        "\n",
        "print(\"Проверка разных значений max_depth с кросс-валидацией (3-fold):\")\n",
        "\n",
        "for depth in max_depth_values:\n",
        "    rf = RandomForestRegressor(n_estimators=300, max_depth=depth, random_state=42, n_jobs=-1)\n",
        "    cv_scores = cross_val_score(rf, X_train, y_train, cv=3, scoring='r2')\n",
        "    mean_score = cv_scores.mean()\n",
        "    results.append({'max_depth': depth, 'mean_r2': mean_score})\n",
        "    print(f\"max_depth={str(depth):4s}: R² (3-fold CV) = {mean_score:.4f}\")\n",
        "\n",
        "# Находим лучший max_depth\n",
        "best_depth = max(results, key=lambda x: x['mean_r2'])['max_depth']\n",
        "best_r2_cv = max(results, key=lambda x: x['mean_r2'])['mean_r2']\n",
        "\n",
        "print(f\"\\nЛучший max_depth = {best_depth} (R² = {best_r2_cv:.4f})\")\n",
        "\n",
        "\n",
        "# Обучаем с лучшими параметрами\n",
        "rf_best = RandomForestRegressor(n_estimators=300, max_depth=best_depth, random_state=42)\n",
        "rf_best.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_best.predict(X_test)\n",
        "\n",
        "print(f\"\\nГипотеза 4: подбор max_depth (n_estimators=300, max_depth={best_depth})\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z_CsZI9PY5r",
        "outputId": "433e95c9-7f13-4988-feda-94461917697b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проверка разных значений max_depth с кросс-валидацией (3-fold):\n",
            "max_depth=10  : R² (3-fold CV) = 0.5634\n",
            "max_depth=20  : R² (3-fold CV) = 0.5348\n",
            "max_depth=30  : R² (3-fold CV) = 0.5325\n",
            "max_depth=None: R² (3-fold CV) = 0.5321\n",
            "\n",
            "Лучший max_depth = 10 (R² = 0.5634)\n",
            "\n",
            "Гипотеза 4: подбор max_depth (n_estimators=300, max_depth=10)\n",
            "MAE: 405039.63\n",
            "RMSE: 513043.37\n",
            "R²: 0.5819\n",
            "MAPE: 39.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод по Гипотезе 4:**\n",
        "\n",
        "max_depth=10 значительно улучшил результаты, R² вырос с 0.5496 до 0.5819, MAE и RMSE заметно снизились, MAPE улучшился с 40.11% до 39.32%, ограничение глубины предотвратило переобучение, кросс-валидация показала, что max_depth=10 лучше всех вариантов.\n",
        "\n",
        "Гипотеза 4 подтверждена, max_depth=10 добавляем в улучшенный бейзлайн."
      ],
      "metadata": {
        "id": "piwNtIBRQZ-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. и d. Формирование улучшенного бейзлайна и обучение модели**"
      ],
      "metadata": {
        "id": "QmhI2efl37xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "\n",
        "# Заполняем пропуски\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "\n",
        "# Улучшение 1: Создаём новые признаки\n",
        "city_mean_salary = df.groupby('City')['Salary_INR'].mean()\n",
        "df['City_Mean_Salary'] = df['City'].map(city_mean_salary)\n",
        "\n",
        "job_mean_salary = df.groupby('Job_Role')['Salary_INR'].mean()\n",
        "df['Job_Mean_Salary'] = df['Job_Role'].map(job_mean_salary)\n",
        "\n",
        "exp_mean_salary = df.groupby('Experience_Level')['Salary_INR'].mean()\n",
        "df['Exp_Mean_Salary'] = df['Experience_Level'].map(exp_mean_salary)\n",
        "\n",
        "df['Demand_Remote_Interaction'] = df['Demand_Index'] * df['Remote_Option_Flag']\n",
        "\n",
        "\n",
        "# Кодируем категориальные признаки\n",
        "for col in ['Company_Name', 'Job_Role', 'Experience_Level', 'City']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Улучшение 2: Обучаем с подобранными гиперпараметрами (n_estimators=300, max_depth=10)\n",
        "rf_improved = RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42)\n",
        "rf_improved.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf_improved.predict(X_test)\n",
        "\n",
        "\n",
        "# Метрики\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "print(\"Улучшенный бейзлайн случайного леса\")\n",
        "print(f\"MAE: {mae:.2f}\")\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMB3UhnFQnKL",
        "outputId": "b32af479-5dee-4474-f369-b007c96bc424"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Улучшенный бейзлайн случайного леса\n",
            "MAE: 405039.63\n",
            "RMSE: 513043.37\n",
            "R²: 0.5819\n",
            "MAPE: 39.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**f. Сравнение результатов с пунктом 2**\n",
        "\n",
        "**Бейзлайн:**\n",
        "\n",
        "MAE: 417367.38\n",
        "\n",
        "RMSE: 536094.91\n",
        "\n",
        "R²: 0.5435\n",
        "\n",
        "MAPE: 40.37%\n",
        "\n",
        "**Улучшенный бейзлайн**\n",
        "\n",
        "MAE: 405039.63\n",
        "\n",
        "RMSE: 513043.37\n",
        "\n",
        "R²: 0.5819\n",
        "\n",
        "MAPE: 39.32%\n"
      ],
      "metadata": {
        "id": "aUozSPQB37xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**g. Выводы**\n",
        "\n",
        "Все метрики улучшились.\n",
        "\n",
        "Создание новых признаков (средние зарплаты по группам) дало модели важную информацию.\n",
        "\n",
        "Подбор гиперпараметров через кросс-валидацию значительно повысил качество.\n",
        "\n",
        "n_estimators=300 вместо 100 дал небольшой прирост.\n",
        "\n",
        "max_depth=10 сильно улучшил результаты, предотвратив переобучение.\n",
        "\n",
        "R² вырос с 0.5435 до 0.5819 - модель стала лучше объяснять данные.\n",
        "\n",
        "MAE и RMSE снизились, модель точнее предсказывает зарплаты.\n"
      ],
      "metadata": {
        "id": "gKR9UuY437xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Имплементация алгоритма машинного обучения"
      ],
      "metadata": {
        "id": "qZcP8GE_1wVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Имплементация случайного леса**"
      ],
      "metadata": {
        "id": "gxfc3ek25f0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTreeRegressorCustom:\n",
        "\n",
        "    class _Node:\n",
        "        __slots__ = (\"is_leaf\", \"pred_value\", \"feature\", \"threshold\", \"left\", \"right\")\n",
        "        def __init__(self, is_leaf, pred_value=None, feature=None, threshold=None, left=None, right=None):\n",
        "            self.is_leaf = is_leaf\n",
        "            self.pred_value = pred_value  # среднее значение для регрессии\n",
        "            self.feature = feature\n",
        "            self.threshold = threshold\n",
        "            self.left = left\n",
        "            self.right = right\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion=\"mse\",\n",
        "                 max_features=\"sqrt\", random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = int(min_samples_split)\n",
        "        self.criterion = criterion\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.rng_ = np.random.RandomState(random_state)\n",
        "\n",
        "        self.root_ = None\n",
        "\n",
        "    def _impurity(self, y, sample_weight=None):\n",
        "        \"\"\"Импурити узла для регрессии (MSE или MAE).\"\"\"\n",
        "        if y.size == 0:\n",
        "            return 0.0\n",
        "\n",
        "        if sample_weight is None:\n",
        "            mean_val = np.mean(y)\n",
        "            if self.criterion == \"mse\":\n",
        "                return np.mean((y - mean_val) ** 2)\n",
        "            elif self.criterion == \"mae\":\n",
        "                return np.mean(np.abs(y - np.median(y)))\n",
        "            else:\n",
        "                raise ValueError(\"нужно 'mse' or 'mae'\")\n",
        "        else:\n",
        "            total_w = sample_weight.sum()\n",
        "            if total_w <= 0:\n",
        "                return 0.0\n",
        "            mean_val = np.sum(y * sample_weight) / total_w\n",
        "            if self.criterion == \"mse\":\n",
        "                return np.sum(sample_weight * (y - mean_val) ** 2) / total_w\n",
        "            elif self.criterion == \"mae\":\n",
        "                weighted_median = self._weighted_median(y, sample_weight)\n",
        "                return np.sum(sample_weight * np.abs(y - weighted_median)) / total_w\n",
        "            else:\n",
        "                raise ValueError(\"нужно 'mse' or 'mae'\")\n",
        "\n",
        "    def _weighted_median(self, values, weights):\n",
        "        \"\"\"Вычисление взвешенной медианы.\"\"\"\n",
        "        sorted_idx = np.argsort(values)\n",
        "        sorted_values = values[sorted_idx]\n",
        "        sorted_weights = weights[sorted_idx]\n",
        "        cumsum = np.cumsum(sorted_weights)\n",
        "        cutoff = cumsum[-1] / 2.0\n",
        "        return sorted_values[np.searchsorted(cumsum, cutoff)]\n",
        "\n",
        "    def _leaf_value(self, y, sample_weight=None):\n",
        "        \"\"\"Возвращает предсказанное значение для листа.\"\"\"\n",
        "        if y.size == 0:\n",
        "            return 0.0\n",
        "\n",
        "        if sample_weight is None:\n",
        "            return np.mean(y)\n",
        "        else:\n",
        "            total_w = sample_weight.sum()\n",
        "            if total_w <= 0:\n",
        "                return 0.0\n",
        "            return np.sum(y * sample_weight) / total_w\n",
        "\n",
        "    def _n_features_to_take(self, n_features):\n",
        "        if self.max_features in (None, \"all\"):\n",
        "            return n_features\n",
        "        if self.max_features == \"sqrt\":\n",
        "            return max(1, int(np.sqrt(n_features)))\n",
        "        if self.max_features == \"log2\":\n",
        "            return max(1, int(np.log2(n_features)))\n",
        "        # Если число\n",
        "        k = int(self.max_features)\n",
        "        return max(1, min(n_features, k))\n",
        "\n",
        "    def _best_split(self, X, y, sample_weight=None):\n",
        "        \"\"\"Ищем лучший сплит: (best_feature, best_threshold, best_gain).\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples < self.min_samples_split:\n",
        "            return None, None, 0.0\n",
        "\n",
        "        parent_imp = self._impurity(y, sample_weight)\n",
        "        if parent_imp <= 1e-12:\n",
        "            return None, None, 0.0\n",
        "\n",
        "        k = self._n_features_to_take(n_features)\n",
        "        feat_candidates = self.rng_.choice(n_features, size=k, replace=False)\n",
        "\n",
        "        best_gain = 0.0\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Для ускорения: считаем веса узла один раз\n",
        "        if sample_weight is None:\n",
        "            total_w = float(n_samples)\n",
        "        else:\n",
        "            total_w = float(sample_weight.sum())\n",
        "            if total_w <= 0:\n",
        "                return None, None, 0.0\n",
        "\n",
        "        for f in feat_candidates:\n",
        "            x = X[:, f]\n",
        "            uniq = np.unique(x)\n",
        "            if uniq.size <= 1:\n",
        "                continue\n",
        "\n",
        "            # Пороги середины между соседними уникальными значениями\n",
        "            thresholds = (uniq[:-1] + uniq[1:]) / 2.0\n",
        "\n",
        "            for thr in thresholds:\n",
        "                left_mask = x <= thr\n",
        "                right_mask = ~left_mask\n",
        "                if not left_mask.any() or not right_mask.any():\n",
        "                    continue\n",
        "\n",
        "                y_left = y[left_mask]\n",
        "                y_right = y[right_mask]\n",
        "\n",
        "                if sample_weight is None:\n",
        "                    w_left = float(y_left.size)\n",
        "                    w_right = float(y_right.size)\n",
        "                    sw_left = None\n",
        "                    sw_right = None\n",
        "                else:\n",
        "                    sw_left = sample_weight[left_mask]\n",
        "                    sw_right = sample_weight[right_mask]\n",
        "                    w_left = float(sw_left.sum())\n",
        "                    w_right = float(sw_right.sum())\n",
        "                    if w_left <= 0 or w_right <= 0:\n",
        "                        continue\n",
        "\n",
        "                imp_left = self._impurity(y_left, sw_left)\n",
        "                imp_right = self._impurity(y_right, sw_right)\n",
        "\n",
        "                child_imp = (w_left / total_w) * imp_left + (w_right / total_w) * imp_right\n",
        "                gain = parent_imp - child_imp\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = f\n",
        "                    best_threshold = thr\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def _build(self, X, y, depth, sample_weight=None):\n",
        "        # Условия остановки\n",
        "        if (self.max_depth is not None) and (depth >= self.max_depth):\n",
        "            pred_val = self._leaf_value(y, sample_weight)\n",
        "            return self._Node(is_leaf=True, pred_value=pred_val)\n",
        "\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            pred_val = self._leaf_value(y, sample_weight)\n",
        "            return self._Node(is_leaf=True, pred_value=pred_val)\n",
        "\n",
        "        # Если все значения одинаковые - лист\n",
        "        if np.std(y) <= 1e-12:\n",
        "            pred_val = self._leaf_value(y, sample_weight)\n",
        "            return self._Node(is_leaf=True, pred_value=pred_val)\n",
        "\n",
        "        f, thr, gain = self._best_split(X, y, sample_weight)\n",
        "        if f is None or gain <= 0:\n",
        "            pred_val = self._leaf_value(y, sample_weight)\n",
        "            return self._Node(is_leaf=True, pred_value=pred_val)\n",
        "\n",
        "        left_mask = X[:, f] <= thr\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        X_left, X_right = X[left_mask], X[right_mask]\n",
        "        y_left, y_right = y[left_mask], y[right_mask]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            sw_left = None\n",
        "            sw_right = None\n",
        "        else:\n",
        "            sw_left = sample_weight[left_mask]\n",
        "            sw_right = sample_weight[right_mask]\n",
        "\n",
        "        left = self._build(X_left, y_left, depth + 1, sw_left)\n",
        "        right = self._build(X_right, y_right, depth + 1, sw_right)\n",
        "\n",
        "        return self._Node(is_leaf=False, feature=f, threshold=thr, left=left, right=right)\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = np.asarray(sample_weight, dtype=float)\n",
        "            if sample_weight.shape[0] != X.shape[0]:\n",
        "                raise ValueError(\"sample_weight должен иметь форму (n_samples,)\")\n",
        "\n",
        "        self.root_ = self._build(X, y, depth=0, sample_weight=sample_weight)\n",
        "        return self\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        while not node.is_leaf:\n",
        "            if x[node.feature] <= node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node.pred_value\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        preds = []\n",
        "        for i in range(X.shape[0]):\n",
        "            pred = self._predict_one(X[i], self.root_)\n",
        "            preds.append(pred)\n",
        "        return np.asarray(preds)\n",
        "\n",
        "\n",
        "class RandomForestRegressorCustom:\n",
        "\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n",
        "                 max_features=\"sqrt\", criterion=\"mse\", random_state=None):\n",
        "        self.n_estimators = int(n_estimators)\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = int(min_samples_split)\n",
        "        self.max_features = max_features\n",
        "        self.criterion = criterion\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.rng_ = np.random.RandomState(random_state)\n",
        "        self.trees_ = []\n",
        "\n",
        "    def _bootstrap_indices(self, n):\n",
        "        return self.rng_.randint(0, n, size=n)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y, dtype=float)\n",
        "\n",
        "        self.trees_ = []\n",
        "        for i in range(self.n_estimators):\n",
        "            idx = self._bootstrap_indices(X.shape[0])\n",
        "            Xb = X[idx]\n",
        "            yb = y[idx]\n",
        "\n",
        "            # Разные random_state для деревьев\n",
        "            tree = DecisionTreeRegressorCustom(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                criterion=self.criterion,\n",
        "                max_features=self.max_features,\n",
        "                random_state=int(self.rng_.randint(0, 1_000_000_000))\n",
        "            )\n",
        "            tree.fit(Xb, yb)\n",
        "            self.trees_.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        predictions = np.zeros((X.shape[0], len(self.trees_)), dtype=float)\n",
        "\n",
        "        for i, tree in enumerate(self.trees_):\n",
        "            predictions[:, i] = tree.predict(X)\n",
        "\n",
        "        return np.mean(predictions, axis=1)"
      ],
      "metadata": {
        "id": "04nnExPeRxSR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. и c. Обучение и оценка качества имплементированной модели с базовым бейзлайном**"
      ],
      "metadata": {
        "id": "y1gvre-Q5f1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Загружаем датасет\n",
        "df_base = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df_base = df_base.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Базовый препроцессинг: Label Encoding для категориальных признаков\n",
        "for col in ['Company_Name', 'Job_Role', 'Experience_Level', 'City']:\n",
        "    le = LabelEncoder()\n",
        "    df_base[col] = le.fit_transform(df_base[col])\n",
        "\n",
        "# Заполняем пропуски медианой\n",
        "df_base = df_base.fillna(df_base.median())\n",
        "\n",
        "# Разделяем на признаки и целевую переменную\n",
        "X_base = df_base.drop('Salary_INR', axis=1)\n",
        "y_base = df_base['Salary_INR']\n",
        "\n",
        "# Train/test разделение\n",
        "X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(\n",
        "    X_base, y_base, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Нормализуем признаки\n",
        "scaler_base = StandardScaler()\n",
        "X_train_base = scaler_base.fit_transform(X_train_base)\n",
        "X_test_base = scaler_base.transform(X_test_base)\n",
        "\n",
        "# Конвертируем в numpy\n",
        "X_train_base = np.array(X_train_base)\n",
        "X_test_base = np.array(X_test_base)\n",
        "y_train_base = np.array(y_train_base)\n",
        "y_test_base = np.array(y_test_base)\n",
        "\n",
        "print(f\"Форма train: {X_train_base.shape}, test: {X_test_base.shape}\")\n",
        "\n",
        "# Обучаем\n",
        "rf_base_custom = RandomForestRegressorCustom(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_base_custom.fit(X_train_base, y_train_base)\n",
        "\n",
        "print(\"модель обучена\")\n",
        "\n",
        "# Предсказания\n",
        "y_pred_base_custom = rf_base_custom.predict(X_test_base)\n",
        "\n",
        "# Метрики\n",
        "print(\"Имплементированный случайный лес (базовый бейзлайн)\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test_base, y_pred_base_custom):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_base, y_pred_base_custom)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test_base, y_pred_base_custom):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test_base - y_pred_base_custom) / y_test_base)) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvZCzUNER5vM",
        "outputId": "61ad1226-7090-4a0d-cd80-49826e879028"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Форма train: (24000, 6), test: (6000, 6)\n",
            "модель обучена\n",
            "Имплементированный случайный лес (базовый бейзлайн)\n",
            "MAE: 417580.50\n",
            "RMSE: 535593.98\n",
            "R²: 0.5443\n",
            "MAPE: 41.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Сравнение реузльтатов с пунктом 2**\n",
        "\n",
        "**Sklearn:**\n",
        "\n",
        "MAE: 417367.38\n",
        "\n",
        "RMSE: 536094.91\n",
        "\n",
        "R²: 0.5435\n",
        "\n",
        "MAPE: 40.37%\n",
        "\n",
        "**Имплементированный:**\n",
        "\n",
        "MAE: 417580.50\n",
        "\n",
        "RMSE: 535593.98\n",
        "\n",
        "R²: 0.5443\n",
        "\n",
        "MAPE: 41.64%\n",
        "\n",
        "Результаты схожи."
      ],
      "metadata": {
        "id": "43EHS5335f1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e. Выводы**\n",
        "\n",
        "Имплементированный случайный лес на базовом бейзлайне показал качество, практически идентичное sklearn. Основные метрики отличаются минимально, что подтверждает корректность реализации алгоритма."
      ],
      "metadata": {
        "id": "_W4RjZfV5f1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**f., g. и h. Добавление техник из улучшенного бейзлайна, обучение и оценка качества имплементированной модели**"
      ],
      "metadata": {
        "id": "YzFXsXXk5f1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "df = pd.read_csv('Job_Market_India.csv')\n",
        "\n",
        "# Убираем ненужные колонки\n",
        "df = df.drop(['Record_Date', 'Salary_Trend_Pct'], axis=1)\n",
        "\n",
        "# Улучшение 1: Удаляем выбросы по зарплате и индексу спроса\n",
        "print(\"До удаления выбросов:\", len(df))\n",
        "df = df[(df['Salary_INR'] > 100000) & (df['Salary_INR'] < 3000000)]\n",
        "df = df[(df['Demand_Index'] >= 0) & (df['Demand_Index'] <= 100)]\n",
        "print(\"После удаления выбросов:\", len(df))\n",
        "\n",
        "# Улучшение 2: Заполняем пропуски только в числовых колонках\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Улучшение 3: Биннинг ТОЛЬКО для Demand_Index (не для целевой переменной!)\n",
        "df['Demand_Index_bin'] = pd.qcut(df['Demand_Index'], q=5, labels=False, duplicates='drop')\n",
        "\n",
        "print(\"Создан биннированный признак:\")\n",
        "print(f\" - Demand_Index_bin (5 интервалов)\")\n",
        "\n",
        "# Улучшение 4: One-hot encoding для категориальных признаков\n",
        "df = pd.get_dummies(df, columns=['Company_Name', 'Job_Role', 'Experience_Level', 'City'], drop_first=False)\n",
        "\n",
        "# Разделяем на признаки и целевую переменную (БЕЗ Salary_INR_bin!)\n",
        "X = df.drop('Salary_INR', axis=1)\n",
        "y = df['Salary_INR']\n",
        "\n",
        "print(f\"\\nКоличество признаков (с улучшениями): {X.shape[1]}\")\n",
        "\n",
        "# Train/test разделение\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Нормализуем данные\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Конвертируем в numpy\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(f\"Форма train: {X_train.shape}, test: {X_test.shape}\")\n",
        "\n",
        "# Обучаем\n",
        "rf_improved = RandomForestRegressorCustom(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_improved.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nмодель обучена\")\n",
        "\n",
        "# Предсказания\n",
        "y_pred = rf_improved.predict(X_test)\n",
        "\n",
        "# Метрики\n",
        "print(\"\\nИмплементированный случайный (улучшенный бейзлайн)\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "print(f\"R²: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"MAPE: {np.mean(np.abs((y_test - y_pred) / y_test)) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yemlRfmEToPE",
        "outputId": "994127ae-c3b7-413a-c01a-3c1b42e37d34"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "До удаления выбросов: 30000\n",
            "После удаления выбросов: 28536\n",
            "Создан биннированный признак:\n",
            " - Demand_Index_bin (5 интервалов)\n",
            "\n",
            "Количество признаков (с улучшениями): 67\n",
            "Форма train: (22828, 67), test: (5708, 67)\n",
            "\n",
            "модель обучена\n",
            "\n",
            "Имплементированный случайный (улучшенный бейзлайн)\n",
            "MAE: 369455.88\n",
            "RMSE: 450168.05\n",
            "R²: 0.4302\n",
            "MAPE: 39.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**i. Сравнение результатов с пунктом 3**\n",
        "\n",
        "**Sklearn:**\n",
        "\n",
        "MAE: 405039.63\n",
        "\n",
        "RMSE: 513043.37\n",
        "\n",
        "R²: 0.5819\n",
        "\n",
        "MAPE: 39.32%\n",
        "\n",
        "**Имплементированный:**\n",
        "\n",
        "MAE: 369455.88\n",
        "\n",
        "RMSE: 450168.05\n",
        "\n",
        "R²: 0.4302\n",
        "\n",
        "MAPE: 39.53%\n",
        "\n",
        "Результаты схожи."
      ],
      "metadata": {
        "id": "L8a7NWhc5f1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**j. Выводы**\n",
        "\n",
        "Моя реализация алгоритма с улучшениями показала неплохие результаты, да чуть хуже модели sklearn, но тем не менее."
      ],
      "metadata": {
        "id": "zbX21Mdt5f1A"
      }
    }
  ]
}